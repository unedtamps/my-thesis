{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro-markdown",
            "metadata": {},
            "source": [
                "# Top2Vec Training with Pre-computed Embeddings (v1)\n",
                "\n",
                "This notebook trains Top2Vec models for each subject (cs, math, physics) using all 6 pre-computed v1 embeddings, computes coherence scores (c_v), and saves results to CSV."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import gc\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from typing import List, Optional\n",
                "from tqdm import tqdm\n",
                "import warnings\n",
                "\n",
                "from top2vec import Top2Vec\n",
                "from sentence_transformers import SentenceTransformer\n",
                "from sklearn.preprocessing import normalize\n",
                "from gensim.corpora import Dictionary\n",
                "from gensim.models.coherencemodel import CoherenceModel\n",
                "from adapters import AutoAdapterModel\n",
                "from transformers import AutoTokenizer\n",
                "import torch\n",
                "\n",
                "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "config-markdown",
            "metadata": {},
            "source": [
                "## Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "VERSION = \"v1\"\n",
                "LIST_SUBJECT = [\"cs\", \"math\", \"physics\"]\n",
                "\n",
                "TRANSFORMERS = [\n",
                "    \"allenai/specter2\",\n",
                "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
                "    \"all-distilroberta-v1\",\n",
                "    \"intfloat/e5-base-v2\",\n",
                "    \"all-mpnet-base-v2\",\n",
                "    \"BAAI/bge-base-en-v1.5\"\n",
                "]\n",
                "\n",
                "EMBEDDING_DIMS = {\n",
                "    \"allenai/specter2\": 768,\n",
                "    \"sentence-transformers/all-MiniLM-L6-v2\": 384,\n",
                "    \"all-distilroberta-v1\": 768,\n",
                "    \"intfloat/e5-base-v2\": 768,\n",
                "    \"all-mpnet-base-v2\": 768,\n",
                "    \"BAAI/bge-base-en-v1.5\": 768\n",
                "}\n",
                "\n",
                "BASE_DIR = Path(\"../../dataset\")\n",
                "EMBEDDING_DIR = Path(\"../bertopic/embedding\")\n",
                "RESULT_DIR = Path(\"./\")\n",
                "MODEL_DIR = Path(\"./transformer\")\n",
                "\n",
                "RESULT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "MODEL_DIR.mkdir(parents=True, exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "helpers-markdown",
            "metadata": {},
            "source": [
                "## Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "helper-functions",
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_specter2():\n",
                "    tokenizer = AutoTokenizer.from_pretrained(\"allenai/specter2_base\")\n",
                "    model = AutoAdapterModel.from_pretrained(\"allenai/specter2_base\")\n",
                "    model.load_adapter(\"allenai/specter2\", source=\"hf\", set_active=True)\n",
                "    model.eval()\n",
                "    if torch.cuda.is_available():\n",
                "        model = model.cuda()\n",
                "    return model, tokenizer\n",
                "\n",
                "\n",
                "def encode_specter2(texts: List[str], model, tokenizer, batch_size: int = 32) -> np.ndarray:\n",
                "    embeddings = []\n",
                "    device = next(model.parameters()).device\n",
                "    for i in tqdm(range(0, len(texts), batch_size), desc=\"  Generating\"):\n",
                "        batch = texts[i:i + batch_size]\n",
                "        inputs = tokenizer(\n",
                "            batch, padding=True, truncation=True,\n",
                "            max_length=512, return_tensors=\"pt\"\n",
                "        ).to(device)\n",
                "        with torch.no_grad():\n",
                "            outputs = model(**inputs)\n",
                "            batch_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
                "        embeddings.append(batch_emb)\n",
                "    return np.vstack(embeddings)\n",
                "\n",
                "\n",
                "def get_model_safe_name(model_name: str) -> str:\n",
                "    return model_name.replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
                "\n",
                "\n",
                "def load_dataset(subject: str) -> Optional[pd.DataFrame]:\n",
                "    file_path = BASE_DIR / subject / \"emb\" / f\"{VERSION}.csv\"\n",
                "    if not file_path.exists():\n",
                "        print(f\"File not found: {file_path}\")\n",
                "        return None\n",
                "    return pd.read_csv(file_path)\n",
                "\n",
                "\n",
                "def load_mmap_embeddings(\n",
                "    mmap_path: str,\n",
                "    num_documents: int,\n",
                "    embedding_dim: int,\n",
                "    dtype: str = \"float32\"\n",
                ") -> Optional[np.ndarray]:\n",
                "    try:\n",
                "        embs = np.array(np.memmap(\n",
                "            mmap_path, dtype=dtype, mode=\"r\",\n",
                "            shape=(num_documents, embedding_dim)\n",
                "        ))\n",
                "        return normalize(embs)\n",
                "    except FileNotFoundError:\n",
                "        print(f\"Embedding not found: {mmap_path}\")\n",
                "        return None\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading embeddings: {e}\")\n",
                "        return None\n",
                "\n",
                "\n",
                "def train_top2vec_with_precomputed(\n",
                "    documents: List[str],\n",
                "    precomputed_embeddings: np.ndarray,\n",
                "    transformer_name: str\n",
                ") -> Top2Vec:\n",
                "    num_docs = len(documents)\n",
                "\n",
                "    if transformer_name == \"allenai/specter2\":\n",
                "        st_model, alle_tokenizer = load_specter2()\n",
                "    else:\n",
                "        st_model = SentenceTransformer(transformer_name)\n",
                "\n",
                "    original_embed_docs = Top2Vec._embed_documents\n",
                "\n",
                "    def patched_embed_documents(self, train_corpus, batch_size):\n",
                "        if len(train_corpus) == num_docs:\n",
                "            print(f\"  âœ… Using pre-computed document embeddings ({num_docs} docs)\")\n",
                "            return precomputed_embeddings\n",
                "        else:\n",
                "            print(f\"  ðŸ“ Encoding vocab with {transformer_name} ({len(train_corpus)} words)\")\n",
                "            if transformer_name == \"allenai/specter2\":\n",
                "                return encode_specter2(train_corpus, st_model, alle_tokenizer, batch_size=batch_size)\n",
                "            return st_model.encode(train_corpus, batch_size=batch_size, show_progress_bar=False)\n",
                "\n",
                "    Top2Vec._embed_documents = patched_embed_documents\n",
                "\n",
                "    model = Top2Vec(\n",
                "        documents=documents,\n",
                "        embedding_model='all-MiniLM-L6-v2',\n",
                "        contextual_top2vec=False,\n",
                "        ngram_vocab=False,\n",
                "    )\n",
                "\n",
                "    Top2Vec._embed_documents = original_embed_docs\n",
                "    del st_model\n",
                "\n",
                "    return model\n",
                "\n",
                "\n",
                "def calculate_coherence(\n",
                "    model: Top2Vec,\n",
                "    texts_tokenized: List[List[str]],\n",
                "    dictionary: Dictionary,\n",
                "    top_n: int = 5\n",
                ") -> float:\n",
                "    num_topics = model.get_num_topics()\n",
                "    topic_words, _, _ = model.get_topics(num_topics)\n",
                "    topic_words_sliced = topic_words[:, :top_n]\n",
                "\n",
                "    cm = CoherenceModel(\n",
                "        topics=topic_words_sliced.tolist(),\n",
                "        texts=texts_tokenized,\n",
                "        dictionary=dictionary,\n",
                "        coherence='c_v',\n",
                "        processes=1\n",
                "    )\n",
                "\n",
                "    return cm.get_coherence()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "load-data-markdown",
            "metadata": {},
            "source": [
                "## Load Datasets & Tokenize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "load-data",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "cs: 165,756 documents loaded\n",
                        "  Tokenizing for coherence...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  cs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 165756/165756 [00:02<00:00, 73711.93it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "math: 126,192 documents loaded\n",
                        "  Tokenizing for coherence...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  math: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 126192/126192 [00:00<00:00, 134207.08it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "physics: 146,311 documents loaded\n",
                        "  Tokenizing for coherence...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  physics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146311/146311 [00:02<00:00, 62706.06it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Total subjects loaded: 3\n"
                    ]
                }
            ],
            "source": [
                "all_data = {}\n",
                "all_texts_tokenized = {}\n",
                "all_dictionaries = {}\n",
                "\n",
                "for subject in LIST_SUBJECT:\n",
                "    df = load_dataset(subject)\n",
                "    if df is not None:\n",
                "        all_data[subject] = df\n",
                "        print(f\"{subject}: {len(df):,} documents loaded\")\n",
                "\n",
                "        print(f\"  Tokenizing for coherence...\")\n",
                "        texts_tokenized = [text.split() for text in tqdm(df['text'].fillna('').tolist(), desc=f\"  {subject}\")]\n",
                "        all_texts_tokenized[subject] = texts_tokenized\n",
                "        all_dictionaries[subject] = Dictionary(texts_tokenized)\n",
                "\n",
                "print(f\"\\nTotal subjects loaded: {len(all_data)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "train-markdown",
            "metadata": {},
            "source": [
                "## Train Top2Vec for Each Subject Ã— Embedding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "train-loop",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "======================================================================\n",
                        "Subject: CS (165,756 documents)\n",
                        "======================================================================\n",
                        "\n",
                        "[allenai/specter2]\n",
                        "  Embedding dim: 768\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "550efba273ac485bac324201e9a09f42",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "There are adapters available but none are activated for the forward pass.\n",
                        "2026-02-11 22:52:18,318 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 22:52:49,899 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 22:52:54,344 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with allenai/specter2 (14689 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 460/460 [00:04<00:00, 95.88it/s] \n",
                        "2026-02-11 22:52:59,152 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (165756 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 22:53:36,241 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 22:53:41,043 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 1\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.2972\n",
                        "  âœ“ Saved to model_results/cs/allenai_specter2_v1\n",
                        "\n",
                        "[sentence-transformers/all-MiniLM-L6-v2]\n",
                        "  Embedding dim: 384\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 22:54:10,259 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 22:54:42,406 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 22:54:46,375 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with sentence-transformers/all-MiniLM-L6-v2 (14689 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 22:54:47,533 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (165756 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 22:55:10,941 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 22:55:15,063 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 808\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.5718\n",
                        "  âœ“ Saved to model_results/cs/sentence_transformers_all_MiniLM_L6_v2_v1\n",
                        "\n",
                        "[all-distilroberta-v1]\n",
                        "  Embedding dim: 768\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 22:57:25,640 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 22:57:57,825 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 22:58:02,056 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with all-distilroberta-v1 (14689 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 22:58:04,207 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (165756 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 22:58:27,514 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 22:58:31,376 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 834\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.4810\n",
                        "  âœ“ Saved to model_results/cs/all_distilroberta_v1_v1\n",
                        "\n",
                        "[intfloat/e5-base-v2]\n",
                        "  Embedding dim: 768\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:00:30,193 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 23:01:02,191 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 23:01:06,066 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with intfloat/e5-base-v2 (14689 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:01:10,189 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (165756 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:01:34,279 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 23:01:38,710 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 1\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.2440\n",
                        "  âœ“ Saved to model_results/cs/intfloat_e5_base_v2_v1\n",
                        "\n",
                        "[all-mpnet-base-v2]\n",
                        "  Embedding dim: 768\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:02:09,025 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 23:02:41,121 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 23:02:44,999 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with all-mpnet-base-v2 (14689 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:02:48,865 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (165756 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:03:12,744 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 23:03:16,595 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 1031\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.4928\n",
                        "  âœ“ Saved to model_results/cs/all_mpnet_base_v2_v1\n",
                        "\n",
                        "[BAAI/bge-base-en-v1.5]\n",
                        "  Embedding dim: 768\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:05:09,519 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 23:05:40,681 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 23:05:44,578 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with BAAI/bge-base-en-v1.5 (14689 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:05:48,714 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (165756 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:06:13,749 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 23:06:17,810 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 92\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.4634\n",
                        "  âœ“ Saved to model_results/cs/BAAI_bge_base_en_v1.5_v1\n",
                        "\n",
                        "======================================================================\n",
                        "Subject: MATH (126,192 documents)\n",
                        "======================================================================\n",
                        "\n",
                        "[allenai/specter2]\n",
                        "  Embedding dim: 768\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0845b8ba128447d49f777d22d0a15534",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "There are adapters available but none are activated for the forward pass.\n",
                        "2026-02-11 23:07:18,417 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 23:07:34,793 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 23:07:38,811 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with allenai/specter2 (9673 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 303/303 [00:02<00:00, 105.96it/s]\n",
                        "2026-02-11 23:07:41,677 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (126192 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:07:58,467 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 23:08:02,025 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 1\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.5680\n",
                        "  âœ“ Saved to model_results/math/allenai_specter2_v1\n",
                        "\n",
                        "[sentence-transformers/all-MiniLM-L6-v2]\n",
                        "  Embedding dim: 384\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:08:16,515 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 23:08:32,647 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 23:08:36,660 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with sentence-transformers/all-MiniLM-L6-v2 (9673 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:08:37,442 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (126192 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:08:52,346 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 23:08:55,144 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 556\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.5464\n",
                        "  âœ“ Saved to model_results/math/sentence_transformers_all_MiniLM_L6_v2_v1\n",
                        "\n",
                        "[all-distilroberta-v1]\n",
                        "  Embedding dim: 768\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:09:38,959 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 23:09:55,189 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 23:09:59,004 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with all-distilroberta-v1 (9673 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:10:00,540 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (126192 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:10:17,451 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 23:10:20,262 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 569\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.4669\n",
                        "  âœ“ Saved to model_results/math/all_distilroberta_v1_v1\n",
                        "\n",
                        "[intfloat/e5-base-v2]\n",
                        "  Embedding dim: 768\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:11:02,120 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 23:11:18,453 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 23:11:22,305 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with intfloat/e5-base-v2 (9673 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:11:24,902 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (126192 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:11:41,597 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 23:11:44,668 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 1\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.3140\n",
                        "  âœ“ Saved to model_results/math/intfloat_e5_base_v2_v1\n",
                        "\n",
                        "[all-mpnet-base-v2]\n",
                        "  Embedding dim: 768\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:11:59,808 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 23:12:16,137 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 23:12:19,780 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with all-mpnet-base-v2 (9673 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:12:22,214 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (126192 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:12:38,150 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 23:12:40,959 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 636\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.4628\n",
                        "  âœ“ Saved to model_results/math/all_mpnet_base_v2_v1\n",
                        "\n",
                        "[BAAI/bge-base-en-v1.5]\n",
                        "  Embedding dim: 768\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:13:20,379 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 23:13:36,522 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 23:13:40,671 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with BAAI/bge-base-en-v1.5 (9673 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:13:43,438 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (126192 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:13:59,812 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 23:14:02,705 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 22\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.5031\n",
                        "  âœ“ Saved to model_results/math/BAAI_bge_base_en_v1.5_v1\n",
                        "\n",
                        "======================================================================\n",
                        "Subject: PHYSICS (146,311 documents)\n",
                        "======================================================================\n",
                        "\n",
                        "[allenai/specter2]\n",
                        "  Embedding dim: 768\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ae5aac363a1948c1a4da9197ee2a276b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "There are adapters available but none are activated for the forward pass.\n",
                        "2026-02-11 23:14:20,925 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 23:14:47,087 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 23:14:51,193 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with allenai/specter2 (14422 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 451/451 [00:04<00:00, 108.12it/s]\n",
                        "2026-02-11 23:14:55,374 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (146311 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:15:15,984 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 23:15:19,605 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 1\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.3205\n",
                        "  âœ“ Saved to model_results/physics/allenai_specter2_v1\n",
                        "\n",
                        "[sentence-transformers/all-MiniLM-L6-v2]\n",
                        "  Embedding dim: 384\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:15:41,668 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 23:16:07,743 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 23:16:11,733 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with sentence-transformers/all-MiniLM-L6-v2 (14422 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:16:12,877 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (146311 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:16:30,528 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 23:16:33,639 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 687\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.6354\n",
                        "  âœ“ Saved to model_results/physics/sentence_transformers_all_MiniLM_L6_v2_v1\n",
                        "\n",
                        "[all-distilroberta-v1]\n",
                        "  Embedding dim: 768\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:17:58,016 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 23:18:23,676 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 23:18:27,537 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with all-distilroberta-v1 (14422 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:18:29,774 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (146311 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:18:49,747 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 23:18:52,853 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 719\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.4549\n",
                        "  âœ“ Saved to model_results/physics/all_distilroberta_v1_v1\n",
                        "\n",
                        "[intfloat/e5-base-v2]\n",
                        "  Embedding dim: 768\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:20:06,653 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 23:20:32,542 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 23:20:36,458 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with intfloat/e5-base-v2 (14422 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:20:40,560 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (146311 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:21:01,491 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 23:21:04,891 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 1\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.1187\n",
                        "  âœ“ Saved to model_results/physics/intfloat_e5_base_v2_v1\n",
                        "\n",
                        "[all-mpnet-base-v2]\n",
                        "  Embedding dim: 768\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:21:27,197 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 23:21:53,309 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 23:21:57,234 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with all-mpnet-base-v2 (14422 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:22:01,086 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (146311 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:22:21,092 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 23:22:24,102 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 810\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.5564\n",
                        "  âœ“ Saved to model_results/physics/all_mpnet_base_v2_v1\n",
                        "\n",
                        "[BAAI/bge-base-en-v1.5]\n",
                        "  Embedding dim: 768\n",
                        "  Training Top2Vec...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:23:35,498 - top2vec - INFO - Pre-processing documents for training\n",
                        "/home/nedo/Kuliah/TA/Program/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "2026-02-11 23:24:00,915 - top2vec - INFO - Downloading all-MiniLM-L6-v2 model\n",
                        "2026-02-11 23:24:05,112 - top2vec - INFO - Creating joint document/word embedding\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ðŸ“ Encoding vocab with BAAI/bge-base-en-v1.5 (14422 words)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:24:09,220 - top2vec - INFO - Creating lower dimension embedding of documents\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  âœ… Using pre-computed document embeddings (146311 docs)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-11 23:24:29,262 - top2vec - INFO - Finding dense areas of documents\n",
                        "2026-02-11 23:24:32,400 - top2vec - INFO - Finding topics\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics found: 76\n",
                        "  Calculating coherence...\n",
                        "  Coherence (c_v): 0.4495\n",
                        "  âœ“ Saved to model_results/physics/BAAI_bge_base_en_v1.5_v1\n",
                        "\n",
                        "======================================================================\n",
                        "All training complete!\n",
                        "======================================================================\n"
                    ]
                }
            ],
            "source": [
                "results = []\n",
                "\n",
                "for subject, df in all_data.items():\n",
                "    print(f\"\\n{'='*70}\")\n",
                "    print(f\"Subject: {subject.upper()} ({len(df):,} documents)\")\n",
                "    print(f\"{'='*70}\")\n",
                "\n",
                "    documents = df['text'].fillna('').tolist()\n",
                "    texts_tokenized = all_texts_tokenized[subject]\n",
                "    dictionary = all_dictionaries[subject]\n",
                "\n",
                "    for model_name in TRANSFORMERS:\n",
                "        model_safe_name = get_model_safe_name(model_name)\n",
                "        emb_dim = EMBEDDING_DIMS[model_name]\n",
                "\n",
                "        embedding_path = str(EMBEDDING_DIR / subject / f\"{model_safe_name}_{VERSION}.mmap\")\n",
                "        model_save_path = str(MODEL_DIR / subject / f\"{model_safe_name}_{VERSION}\")\n",
                "\n",
                "        print(f\"\\n[{model_name}]\")\n",
                "        print(f\"  Embedding dim: {emb_dim}\")\n",
                "\n",
                "        # Load existing model if available\n",
                "        if os.path.exists(model_save_path):\n",
                "            print(f\"  Loading existing model from {model_save_path}\")\n",
                "            try:\n",
                "                model = Top2Vec.load(model_save_path)\n",
                "                n_topics = model.get_num_topics()\n",
                "                print(f\"  Topics: {n_topics}\")\n",
                "\n",
                "                print(f\"  Calculating coherence...\")\n",
                "                coherence = calculate_coherence(model, texts_tokenized, dictionary)\n",
                "                print(f\"  Coherence (c_v): {coherence:.4f}\")\n",
                "\n",
                "                results.append({\n",
                "                    \"subject\": subject,\n",
                "                    \"model\": model_name,\n",
                "                    \"n_topics\": n_topics,\n",
                "                    \"coherence\": coherence\n",
                "                })\n",
                "                del model\n",
                "                gc.collect()\n",
                "                continue\n",
                "            except Exception as e:\n",
                "                print(f\"  Failed to load, retraining: {e}\")\n",
                "\n",
                "        # Train new model\n",
                "        embeddings = load_mmap_embeddings(embedding_path, len(documents), emb_dim)\n",
                "\n",
                "        if embeddings is None:\n",
                "            print(f\"  âœ— Skipping (no embeddings)\")\n",
                "            results.append({\n",
                "                \"subject\": subject,\n",
                "                \"model\": model_name,\n",
                "                \"n_topics\": None,\n",
                "                \"coherence\": None,\n",
                "                \"error\": \"embeddings not found\"\n",
                "            })\n",
                "            continue\n",
                "\n",
                "        try:\n",
                "            print(f\"  Training Top2Vec...\")\n",
                "            model = train_top2vec_with_precomputed(documents, embeddings, model_name)\n",
                "\n",
                "            n_topics = model.get_num_topics()\n",
                "            print(f\"  Topics found: {n_topics}\")\n",
                "\n",
                "            print(f\"  Calculating coherence...\")\n",
                "            coherence = calculate_coherence(model, texts_tokenized, dictionary)\n",
                "            print(f\"  Coherence (c_v): {coherence:.4f}\")\n",
                "\n",
                "            # Save model\n",
                "            model.save(model_save_path)\n",
                "            print(f\"  âœ“ Saved to {model_save_path}\")\n",
                "\n",
                "            results.append({\n",
                "                \"subject\": subject,\n",
                "                \"model\": model_name,\n",
                "                \"n_topics\": n_topics,\n",
                "                \"coherence\": coherence\n",
                "            })\n",
                "\n",
                "            del embeddings, model\n",
                "            gc.collect()\n",
                "\n",
                "        except Exception as e:\n",
                "            print(f\"  âœ— Error: {e}\")\n",
                "            results.append({\n",
                "                \"subject\": subject,\n",
                "                \"model\": model_name,\n",
                "                \"n_topics\": None,\n",
                "                \"coherence\": None,\n",
                "                \"error\": str(e)\n",
                "            })\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"All training complete!\")\n",
                "print(\"=\"*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "results-markdown",
            "metadata": {},
            "source": [
                "## Save & Display Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "save-results",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "======================================================================\n",
                        "COHERENCE SCORE SUMMARY\n",
                        "======================================================================\n",
                        "\n",
                        "subject                                     cs    math  physics    mean\n",
                        "model                                                                  \n",
                        "sentence-transformers/all-MiniLM-L6-v2  0.5718  0.5464   0.6354  0.5846\n",
                        "all-mpnet-base-v2                       0.4928  0.4628   0.5564  0.5040\n",
                        "BAAI/bge-base-en-v1.5                   0.4634  0.5031   0.4495  0.4720\n",
                        "all-distilroberta-v1                    0.4810  0.4669   0.4549  0.4676\n",
                        "allenai/specter2                        0.2972  0.5680   0.3205  0.3952\n",
                        "intfloat/e5-base-v2                     0.2440  0.3140   0.1187  0.2256\n",
                        "\n",
                        "Results saved to: coherence_results_v1.csv\n"
                    ]
                }
            ],
            "source": [
                "results_df = pd.DataFrame(results)\n",
                "csv_path = RESULT_DIR / f\"coherence_results_{VERSION}.csv\"\n",
                "results_df.to_csv(csv_path, index=False)\n",
                "\n",
                "print(f\"\\n{'='*70}\")\n",
                "print(\"COHERENCE SCORE SUMMARY\")\n",
                "print(f\"{'='*70}\\n\")\n",
                "\n",
                "if len(results_df) > 0 and results_df['coherence'].notna().any():\n",
                "    valid = results_df.dropna(subset=['coherence'])\n",
                "    pivot = valid.pivot(index='model', columns='subject', values='coherence')\n",
                "    pivot['mean'] = pivot.mean(axis=1)\n",
                "    pivot = pivot.sort_values('mean', ascending=False)\n",
                "    print(pivot.round(4))\n",
                "\n",
                "print(f\"\\nResults saved to: {csv_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "ranking",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Top Models per Subject:\n",
                        "--------------------------------------------------------------------------------------------------------------\n",
                        "CS         | #1 | sentence-transformers/all-MiniLM-L6-v2        | 0.5718\n",
                        "CS         | #2 | all-mpnet-base-v2                             | 0.4928\n",
                        "CS         | #3 | all-distilroberta-v1                          | 0.4810\n",
                        "CS         | #4 | BAAI/bge-base-en-v1.5                         | 0.4634\n",
                        "CS         | #5 | allenai/specter2                              | 0.2972\n",
                        "CS         | #6 | intfloat/e5-base-v2                           | 0.2440\n",
                        "--------------------------------------------------------------------------------------------------------------\n",
                        "MATH       | #1 | allenai/specter2                              | 0.5680\n",
                        "MATH       | #2 | sentence-transformers/all-MiniLM-L6-v2        | 0.5464\n",
                        "MATH       | #3 | BAAI/bge-base-en-v1.5                         | 0.5031\n",
                        "MATH       | #4 | all-distilroberta-v1                          | 0.4669\n",
                        "MATH       | #5 | all-mpnet-base-v2                             | 0.4628\n",
                        "MATH       | #6 | intfloat/e5-base-v2                           | 0.3140\n",
                        "--------------------------------------------------------------------------------------------------------------\n",
                        "PHYSICS    | #1 | sentence-transformers/all-MiniLM-L6-v2        | 0.6354\n",
                        "PHYSICS    | #2 | all-mpnet-base-v2                             | 0.5564\n",
                        "PHYSICS    | #3 | all-distilroberta-v1                          | 0.4549\n",
                        "PHYSICS    | #4 | BAAI/bge-base-en-v1.5                         | 0.4495\n",
                        "PHYSICS    | #5 | allenai/specter2                              | 0.3205\n",
                        "PHYSICS    | #6 | intfloat/e5-base-v2                           | 0.1187\n",
                        "--------------------------------------------------------------------------------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\nTop Models per Subject:\")\n",
                "print(\"-\" * 110)\n",
                "\n",
                "for subject in LIST_SUBJECT:\n",
                "    subject_results = results_df[results_df['subject'] == subject]\n",
                "    if len(subject_results) > 0 and subject_results['coherence'].notna().any():\n",
                "        top_models = subject_results.nlargest(6, 'coherence')\n",
                "        for rank, (_, row) in enumerate(top_models.iterrows(), 1):\n",
                "            print(f\"{subject.upper():10s} | #{rank} | {row['model']:45s} | {row['coherence']:.4f}\")\n",
                "        print(\"-\" * 110)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
