{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro-markdown",
            "metadata": {},
            "source": [
                "# BERTopic Hyperparameter Tuning (Grid Search)\n",
                "\n",
                "This notebook performs grid search over UMAP and HDBSCAN hyperparameters\n",
                "using the best embedding model per subject:\n",
                "- **CS & Physics**: all-distilroberta-v1\n",
                "- **Math**: BAAI/bge-base-en-v1.5\n",
                "\n",
                "Results and best models are saved to the `tunning/` directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import gc\n",
                "import itertools\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from typing import List, Optional, Tuple\n",
                "from tqdm import tqdm\n",
                "import warnings\n",
                "\n",
                "from umap import UMAP\n",
                "from hdbscan import HDBSCAN\n",
                "from bertopic import BERTopic\n",
                "from gensim.utils import simple_preprocess\n",
                "from gensim.corpora import Dictionary\n",
                "from gensim.models import CoherenceModel\n",
                "\n",
                "pd.set_option('display.max_colwidth', None)\n",
                "warnings.filterwarnings(\"ignore\", category=SyntaxWarning)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "config",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model per subject:\n",
                        "  cs: all-distilroberta-v1\n",
                        "  math: BAAI/bge-base-en-v1.5\n",
                        "  physics: all-distilroberta-v1\n",
                        "\n",
                        "Grid search: 54 combinations per subject\n",
                        "Total trials: 162\n",
                        "Results will be saved to: tunning/hdbscan_v1\n"
                    ]
                }
            ],
            "source": [
                "VERSION = \"v1\"\n",
                "LIST_SUBJECT = [\"cs\", \"math\", \"physics\"]\n",
                "\n",
                "SUBJECT_MODEL = {\n",
                "    \"cs\":      {\"name\": \"all-distilroberta-v1\",  \"safe_name\": \"all_distilroberta_v1\",    \"dim\": 768},\n",
                "    \"math\":    {\"name\": \"BAAI/bge-base-en-v1.5\", \"safe_name\": \"BAAI_bge_base_en_v1.5\",   \"dim\": 768},\n",
                "    \"physics\": {\"name\": \"all-distilroberta-v1\",  \"safe_name\": \"all_distilroberta_v1\",    \"dim\": 768},\n",
                "}\n",
                "\n",
                "PARAM_GRID = {\n",
                "    \"min_cluster_size\": [100, 150, 200],\n",
                "    \"min_samples\": [5, 10, 20],\n",
                "    \"n_neighbors\": [10, 15, 25],\n",
                "    \"n_components\": [5, 10],\n",
                "}\n",
                "\n",
                "BASE_DIR = Path(\"../../dataset\")\n",
                "EMBEDDING_DIR = Path(\"./embedding\")\n",
                "TUNING_DIR = Path(f\"./tunning/hdbscan_{VERSION}\")\n",
                "\n",
                "TUNING_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "total_combos = 1\n",
                "for values in PARAM_GRID.values():\n",
                "    total_combos *= len(values)\n",
                "\n",
                "print(\"Model per subject:\")\n",
                "for subj, info in SUBJECT_MODEL.items():\n",
                "    print(f\"  {subj}: {info['name']}\")\n",
                "print(f\"\\nGrid search: {total_combos} combinations per subject\")\n",
                "print(f\"Total trials: {total_combos * len(LIST_SUBJECT)}\")\n",
                "print(f\"Results will be saved to: {TUNING_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "helper-functions",
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_dataset(subject: str) -> pd.DataFrame:\n",
                "    file_path = BASE_DIR / subject / \"emb\" / f\"{VERSION}.csv\"\n",
                "    if not file_path.exists():\n",
                "        print(f\"File not found: {file_path}\")\n",
                "        return None\n",
                "    return pd.read_csv(file_path)\n",
                "\n",
                "\n",
                "def load_mmap_embeddings(\n",
                "    mmap_path: str,\n",
                "    num_documents: int,\n",
                "    embedding_dim: int,\n",
                "    dtype: str = \"float32\"\n",
                ") -> Optional[np.memmap]:\n",
                "    try:\n",
                "        return np.memmap(\n",
                "            mmap_path, dtype=dtype, mode=\"r\",\n",
                "            shape=(num_documents, embedding_dim)\n",
                "        )\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading embeddings: {e}\")\n",
                "        return None\n",
                "\n",
                "\n",
                "def tokenize_for_coherence(text: str) -> List[str]:\n",
                "    return [token for token in simple_preprocess(str(text), deacc=True)]\n",
                "\n",
                "\n",
                "def calculate_coherence(\n",
                "    topic_model: BERTopic,\n",
                "    texts_tokenized: List[List[str]],\n",
                "    dictionary: Dictionary,\n",
                "    top_n: int = 10\n",
                ") -> float:\n",
                "    topics_list = []\n",
                "    for topic_id in topic_model.get_topics().keys():\n",
                "        if topic_id == -1:\n",
                "            continue\n",
                "        topic_words = [word for word, _ in topic_model.get_topic(topic_id)[:top_n]]\n",
                "        topics_list.append(topic_words)\n",
                "\n",
                "    if not topics_list:\n",
                "        return 0.0\n",
                "\n",
                "    cm = CoherenceModel(\n",
                "        topics=topics_list,\n",
                "        texts=texts_tokenized,\n",
                "        dictionary=dictionary,\n",
                "        coherence='c_v',\n",
                "        processes=1\n",
                "    )\n",
                "    return cm.get_coherence()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "load-data",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "cs: 165,756 documents loaded\n",
                        "  Tokenizing for coherence...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  cs: 100%|██████████| 165756/165756 [00:27<00:00, 6025.01it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Embeddings loaded: (165756, 768) (model: all-distilroberta-v1)\n",
                        "math: 157,085 documents loaded\n",
                        "  Tokenizing for coherence...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  math: 100%|██████████| 157085/157085 [00:18<00:00, 8678.59it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Embeddings loaded: (157085, 768) (model: BAAI/bge-base-en-v1.5)\n",
                        "physics: 146,311 documents loaded\n",
                        "  Tokenizing for coherence...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  physics: 100%|██████████| 146311/146311 [00:22<00:00, 6571.69it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Embeddings loaded: (146311, 768) (model: all-distilroberta-v1)\n",
                        "\n",
                        "Subjects ready: ['cs', 'math', 'physics']\n"
                    ]
                }
            ],
            "source": [
                "all_data = {}\n",
                "all_texts_tokenized = {}\n",
                "all_dictionaries = {}\n",
                "all_embeddings = {}\n",
                "\n",
                "for subject in LIST_SUBJECT:\n",
                "    df = load_dataset(subject)\n",
                "    if df is None:\n",
                "        continue\n",
                "\n",
                "    all_data[subject] = df\n",
                "    texts = df[\"text\"].fillna(\"\").tolist()\n",
                "    print(f\"{subject}: {len(df):,} documents loaded\")\n",
                "\n",
                "    print(f\"  Tokenizing for coherence...\")\n",
                "    texts_tokenized = [tokenize_for_coherence(t) for t in tqdm(texts, desc=f\"  {subject}\")]\n",
                "    all_texts_tokenized[subject] = texts_tokenized\n",
                "    all_dictionaries[subject] = Dictionary(texts_tokenized)\n",
                "\n",
                "    model_info = SUBJECT_MODEL[subject]\n",
                "    emb_path = str(EMBEDDING_DIR / subject / f\"{model_info['safe_name']}_{VERSION}.mmap\")\n",
                "    embeddings = load_mmap_embeddings(emb_path, len(texts), model_info['dim'])\n",
                "    if embeddings is not None:\n",
                "        all_embeddings[subject] = embeddings\n",
                "        print(f\"  Embeddings loaded: {embeddings.shape} (model: {model_info['name']})\")\n",
                "    else:\n",
                "        print(f\"  ⚠ Failed to load embeddings from {emb_path}\")\n",
                "\n",
                "print(f\"\\nSubjects ready: {list(all_data.keys())}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "grid-search",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 108 existing results from tunning/hdbscan_v1/coherence_results.csv\n",
                        "\n",
                        "================================================================================\n",
                        "Subject: CS (165,756 documents) | Model: all-distilroberta-v1\n",
                        "================================================================================\n",
                        "Combinations to run: 0 / 54\n",
                        "\n",
                        "================================================================================\n",
                        "Subject: MATH (157,085 documents) | Model: BAAI/bge-base-en-v1.5\n",
                        "================================================================================\n",
                        "Combinations to run: 54 / 54\n",
                        "\n",
                        "[1/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 254 | Coherence: 0.7091 | Outliers: 43.88%\n",
                        "\n",
                        "[2/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 250 | Coherence: 0.7053 | Outliers: 44.73%\n",
                        "\n",
                        "[3/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 225 | Coherence: 0.7072 | Outliers: 45.22%\n",
                        "\n",
                        "[4/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 233 | Coherence: 0.7114 | Outliers: 47.00%\n",
                        "\n",
                        "[5/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 205 | Coherence: 0.7158 | Outliers: 49.04%\n",
                        "\n",
                        "[6/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 218 | Coherence: 0.7177 | Outliers: 50.91%\n",
                        "\n",
                        "[7/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 233 | Coherence: 0.7059 | Outliers: 42.16%\n",
                        "\n",
                        "[8/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 222 | Coherence: 0.7025 | Outliers: 42.23%\n",
                        "\n",
                        "[9/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 212 | Coherence: 0.7200 | Outliers: 46.35%\n",
                        "\n",
                        "[10/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 215 | Coherence: 0.7078 | Outliers: 45.84%\n",
                        "\n",
                        "[11/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 195 | Coherence: 0.7173 | Outliers: 50.35%\n",
                        "\n",
                        "[12/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 203 | Coherence: 0.7177 | Outliers: 51.47%\n",
                        "\n",
                        "[13/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 231 | Coherence: 0.7115 | Outliers: 46.29%\n",
                        "\n",
                        "[14/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 223 | Coherence: 0.7052 | Outliers: 46.37%\n",
                        "\n",
                        "[15/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 200 | Coherence: 0.7162 | Outliers: 47.85%\n",
                        "\n",
                        "[16/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 191 | Coherence: 0.7180 | Outliers: 46.08%\n",
                        "\n",
                        "[17/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 194 | Coherence: 0.7190 | Outliers: 52.88%\n",
                        "\n",
                        "[18/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 192 | Coherence: 0.7203 | Outliers: 52.28%\n",
                        "\n",
                        "[19/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 176 | Coherence: 0.7155 | Outliers: 43.92%\n",
                        "\n",
                        "[20/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 183 | Coherence: 0.7095 | Outliers: 43.59%\n",
                        "\n",
                        "[21/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 158 | Coherence: 0.7204 | Outliers: 45.14%\n",
                        "\n",
                        "[22/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 162 | Coherence: 0.7128 | Outliers: 47.91%\n",
                        "\n",
                        "[23/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 150 | Coherence: 0.7127 | Outliers: 49.01%\n",
                        "\n",
                        "[24/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 140 | Coherence: 0.7104 | Outliers: 48.66%\n",
                        "\n",
                        "[25/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 170 | Coherence: 0.7121 | Outliers: 43.71%\n",
                        "\n",
                        "[26/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 164 | Coherence: 0.7084 | Outliers: 44.23%\n",
                        "\n",
                        "[27/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 150 | Coherence: 0.7229 | Outliers: 44.53%\n",
                        "\n",
                        "[28/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 158 | Coherence: 0.7155 | Outliers: 45.54%\n",
                        "\n",
                        "[29/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 134 | Coherence: 0.7211 | Outliers: 48.63%\n",
                        "\n",
                        "[30/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 137 | Coherence: 0.7134 | Outliers: 49.18%\n",
                        "\n",
                        "[31/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 168 | Coherence: 0.7125 | Outliers: 46.21%\n",
                        "\n",
                        "[32/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 164 | Coherence: 0.7139 | Outliers: 45.05%\n",
                        "\n",
                        "[33/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 139 | Coherence: 0.7174 | Outliers: 44.87%\n",
                        "\n",
                        "[34/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 146 | Coherence: 0.7161 | Outliers: 45.92%\n",
                        "\n",
                        "[35/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 140 | Coherence: 0.7146 | Outliers: 50.76%\n",
                        "\n",
                        "[36/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 145 | Coherence: 0.7179 | Outliers: 53.19%\n",
                        "\n",
                        "[37/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 140 | Coherence: 0.7105 | Outliers: 43.77%\n",
                        "\n",
                        "[38/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 128 | Coherence: 0.7072 | Outliers: 44.67%\n",
                        "\n",
                        "[39/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 114 | Coherence: 0.7058 | Outliers: 44.05%\n",
                        "\n",
                        "[40/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 123 | Coherence: 0.7092 | Outliers: 45.90%\n",
                        "\n",
                        "[41/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 121 | Coherence: 0.7157 | Outliers: 49.47%\n",
                        "\n",
                        "[42/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 118 | Coherence: 0.7076 | Outliers: 47.13%\n",
                        "\n",
                        "[43/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 138 | Coherence: 0.7151 | Outliers: 44.66%\n",
                        "\n",
                        "[44/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 117 | Coherence: 0.7041 | Outliers: 43.67%\n",
                        "\n",
                        "[45/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 120 | Coherence: 0.7146 | Outliers: 45.69%\n",
                        "\n",
                        "[46/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 120 | Coherence: 0.7070 | Outliers: 44.66%\n",
                        "\n",
                        "[47/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 112 | Coherence: 0.7160 | Outliers: 48.54%\n",
                        "\n",
                        "[48/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 107 | Coherence: 0.7062 | Outliers: 47.82%\n",
                        "\n",
                        "[49/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 125 | Coherence: 0.7182 | Outliers: 44.84%\n",
                        "\n",
                        "[50/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 118 | Coherence: 0.7052 | Outliers: 42.18%\n",
                        "\n",
                        "[51/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 115 | Coherence: 0.7146 | Outliers: 46.54%\n",
                        "\n",
                        "[52/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 114 | Coherence: 0.7085 | Outliers: 45.25%\n",
                        "\n",
                        "[53/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 117 | Coherence: 0.7088 | Outliers: 47.14%\n",
                        "\n",
                        "[54/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 113 | Coherence: 0.7140 | Outliers: 49.62%\n",
                        "\n",
                        "================================================================================\n",
                        "Subject: PHYSICS (146,311 documents) | Model: all-distilroberta-v1\n",
                        "================================================================================\n",
                        "Combinations to run: 0 / 54\n",
                        "\n",
                        "================================================================================\n",
                        "Grid search complete! Results saved to: tunning/hdbscan_v1/coherence_results.csv\n",
                        "Total trials: 162\n",
                        "================================================================================\n"
                    ]
                }
            ],
            "source": [
                "param_keys = list(PARAM_GRID.keys())\n",
                "param_values = list(PARAM_GRID.values())\n",
                "all_combos = list(itertools.product(*param_values))\n",
                "\n",
                "results = []\n",
                "csv_path = TUNING_DIR / \"coherence_results.csv\"\n",
                "\n",
                "existing_results = set()\n",
                "if csv_path.exists():\n",
                "    existing_df = pd.read_csv(csv_path)\n",
                "    for _, row in existing_df.iterrows():\n",
                "        key = (row['subject'], row['min_cluster_size'], row['min_samples'],\n",
                "               row['n_neighbors'], row['n_components'])\n",
                "        existing_results.add(key)\n",
                "    results = existing_df.to_dict('records')\n",
                "    print(f\"Loaded {len(results)} existing results from {csv_path}\")\n",
                "\n",
                "for subject in LIST_SUBJECT:\n",
                "    if subject not in all_embeddings:\n",
                "        print(f\"\\nSkipping {subject} (no embeddings)\")\n",
                "        continue\n",
                "\n",
                "    model_info = SUBJECT_MODEL[subject]\n",
                "    texts = all_data[subject][\"text\"].fillna(\"\").tolist()\n",
                "    embeddings = all_embeddings[subject]\n",
                "    texts_tokenized = all_texts_tokenized[subject]\n",
                "    dictionary = all_dictionaries[subject]\n",
                "\n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(f\"Subject: {subject.upper()} ({len(texts):,} documents) | Model: {model_info['name']}\")\n",
                "    print(f\"{'='*80}\")\n",
                "\n",
                "    subject_combos = [\n",
                "        combo for combo in all_combos\n",
                "        if (subject, *combo) not in existing_results\n",
                "    ]\n",
                "    print(f\"Combinations to run: {len(subject_combos)} / {len(all_combos)}\")\n",
                "\n",
                "    for i, combo in enumerate(subject_combos, 1):\n",
                "        params = dict(zip(param_keys, combo))\n",
                "\n",
                "        print(f\"\\n[{i}/{len(subject_combos)}] {params}\")\n",
                "\n",
                "        try:\n",
                "            umap_model = UMAP(\n",
                "                n_neighbors=params[\"n_neighbors\"],\n",
                "                n_components=params[\"n_components\"],\n",
                "                metric=\"cosine\",\n",
                "                random_state=42,\n",
                "                min_dist=0.0,\n",
                "                verbose=False\n",
                "            )\n",
                "\n",
                "            hdbscan_model = HDBSCAN(\n",
                "                min_cluster_size=params[\"min_cluster_size\"],\n",
                "                min_samples=params[\"min_samples\"],\n",
                "                metric=\"euclidean\",\n",
                "                cluster_selection_method=\"eom\",\n",
                "                prediction_data=True\n",
                "            )\n",
                "\n",
                "            topic_model = BERTopic(\n",
                "                umap_model=umap_model,\n",
                "                hdbscan_model=hdbscan_model,\n",
                "                calculate_probabilities=False,\n",
                "                verbose=False\n",
                "            )\n",
                "\n",
                "            topics, _ = topic_model.fit_transform(texts, embeddings=embeddings)\n",
                "            n_topics = len(topic_model.get_topic_info()) - 1\n",
                "            outlier_count = sum(1 for t in topics if t == -1)\n",
                "            outlier_ratio = outlier_count / len(topics)\n",
                "\n",
                "            coherence = calculate_coherence(topic_model, texts_tokenized, dictionary)\n",
                "\n",
                "            print(f\"  Topics: {n_topics} | Coherence: {coherence:.4f} | Outliers: {outlier_ratio:.2%}\")\n",
                "\n",
                "            results.append({\n",
                "                \"subject\": subject,\n",
                "                \"model\": model_info[\"name\"],\n",
                "                \"min_cluster_size\": params[\"min_cluster_size\"],\n",
                "                \"min_samples\": params[\"min_samples\"],\n",
                "                \"n_neighbors\": params[\"n_neighbors\"],\n",
                "                \"n_components\": params[\"n_components\"],\n",
                "                \"n_topics\": n_topics,\n",
                "                \"coherence\": coherence,\n",
                "                \"outlier_ratio\": outlier_ratio\n",
                "            })\n",
                "\n",
                "            del topic_model\n",
                "            gc.collect()\n",
                "\n",
                "        except Exception as e:\n",
                "            print(f\"  ✗ Error: {e}\")\n",
                "            results.append({\n",
                "                \"subject\": subject,\n",
                "                \"model\": model_info[\"name\"],\n",
                "                \"min_cluster_size\": params[\"min_cluster_size\"],\n",
                "                \"min_samples\": params[\"min_samples\"],\n",
                "                \"n_neighbors\": params[\"n_neighbors\"],\n",
                "                \"n_components\": params[\"n_components\"],\n",
                "                \"n_topics\": None,\n",
                "                \"coherence\": None,\n",
                "                \"outlier_ratio\": None\n",
                "            })\n",
                "\n",
                "        results_df = pd.DataFrame(results)\n",
                "        results_df.to_csv(csv_path, index=False)\n",
                "\n",
                "print(f\"\\n{'='*80}\")\n",
                "print(f\"Grid search complete! Results saved to: {csv_path}\")\n",
                "print(f\"Total trials: {len(results)}\")\n",
                "print(f\"{'='*80}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "show-results",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Best Hyperparameters per Subject\n",
                        "====================================================================================================\n",
                        "\n",
                        "CS (model: all-distilroberta-v1):\n",
                        "  Best Coherence: 0.7364\n",
                        "  Topics: 261\n",
                        "  Outlier Ratio: 40.55%\n",
                        "  Parameters:\n",
                        "    min_cluster_size = 100\n",
                        "    min_samples      = 10\n",
                        "    n_neighbors      = 25\n",
                        "    n_components     = 5\n",
                        "\n",
                        "  Top 5 Configurations:\n",
                        "  Rank  Coherence   Topics   Outliers   min_cluster   min_samples   n_neighbors   n_components\n",
                        "  ------------------------------------------------------------------------------------------\n",
                        "  #1    0.7364     261      40.55%     100           10            25            5\n",
                        "  #2    0.7352     202      36.52%     150           5             15            5\n",
                        "  #3    0.7345     243      41.23%     100           20            25            5\n",
                        "  #4    0.7340     256      37.30%     100           5             25            10\n",
                        "  #5    0.7332     273      37.47%     100           10            15            5\n",
                        "----------------------------------------------------------------------------------------------------\n",
                        "\n",
                        "MATH (model: BAAI/bge-base-en-v1.5):\n",
                        "  Best Coherence: 0.7229\n",
                        "  Topics: 150\n",
                        "  Outlier Ratio: 44.53%\n",
                        "  Parameters:\n",
                        "    min_cluster_size = 150\n",
                        "    min_samples      = 10\n",
                        "    n_neighbors      = 15\n",
                        "    n_components     = 5\n",
                        "\n",
                        "  Top 5 Configurations:\n",
                        "  Rank  Coherence   Topics   Outliers   min_cluster   min_samples   n_neighbors   n_components\n",
                        "  ------------------------------------------------------------------------------------------\n",
                        "  #1    0.7229     150      44.53%     150           10            15            5\n",
                        "  #2    0.7211     134      48.63%     150           10            25            5\n",
                        "  #3    0.7204     158      45.14%     150           5             15            5\n",
                        "  #4    0.7203     192      52.28%     100           20            25            10\n",
                        "  #5    0.7200     212      46.35%     100           10            15            5\n",
                        "----------------------------------------------------------------------------------------------------\n",
                        "\n",
                        "PHYSICS (model: all-distilroberta-v1):\n",
                        "  Best Coherence: 0.7466\n",
                        "  Topics: 232\n",
                        "  Outlier Ratio: 45.30%\n",
                        "  Parameters:\n",
                        "    min_cluster_size = 100\n",
                        "    min_samples      = 5\n",
                        "    n_neighbors      = 25\n",
                        "    n_components     = 10\n",
                        "\n",
                        "  Top 5 Configurations:\n",
                        "  Rank  Coherence   Topics   Outliers   min_cluster   min_samples   n_neighbors   n_components\n",
                        "  ------------------------------------------------------------------------------------------\n",
                        "  #1    0.7466     232      45.30%     100           5             25            10\n",
                        "  #2    0.7460     267      40.42%     100           5             10            5\n",
                        "  #3    0.7458     177      39.88%     150           10            10            10\n",
                        "  #4    0.7456     169      44.26%     150           5             25            5\n",
                        "  #5    0.7453     195      44.85%     100           20            25            10\n",
                        "----------------------------------------------------------------------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "results_df = pd.read_csv(csv_path)\n",
                "\n",
                "print(\"\\nBest Hyperparameters per Subject\")\n",
                "print(\"=\" * 100)\n",
                "\n",
                "for subject in LIST_SUBJECT:\n",
                "    subject_df = results_df[results_df['subject'] == subject].dropna(subset=['coherence'])\n",
                "    if len(subject_df) == 0:\n",
                "        continue\n",
                "\n",
                "    best = subject_df.loc[subject_df['coherence'].idxmax()]\n",
                "    model_name = SUBJECT_MODEL[subject]['name']\n",
                "\n",
                "    print(f\"\\n{subject.upper()} (model: {model_name}):\")\n",
                "    print(f\"  Best Coherence: {best['coherence']:.4f}\")\n",
                "    print(f\"  Topics: {int(best['n_topics'])}\")\n",
                "    print(f\"  Outlier Ratio: {best['outlier_ratio']:.2%}\")\n",
                "    print(f\"  Parameters:\")\n",
                "    print(f\"    min_cluster_size = {int(best['min_cluster_size'])}\")\n",
                "    print(f\"    min_samples      = {int(best['min_samples'])}\")\n",
                "    print(f\"    n_neighbors      = {int(best['n_neighbors'])}\")\n",
                "    print(f\"    n_components     = {int(best['n_components'])}\")\n",
                "\n",
                "    top5 = subject_df.nlargest(5, 'coherence')\n",
                "    print(f\"\\n  Top 5 Configurations:\")\n",
                "    print(f\"  {'Rank':<5} {'Coherence':<11} {'Topics':<8} {'Outliers':<10} {'min_cluster':<13} {'min_samples':<13} {'n_neighbors':<13} {'n_components'}\")\n",
                "    print(f\"  {'-'*90}\")\n",
                "    for rank, (_, row) in enumerate(top5.iterrows(), 1):\n",
                "        print(f\"  #{rank:<4} {row['coherence']:.4f}     {int(row['n_topics']):<8} {row['outlier_ratio']:.2%}     {int(row['min_cluster_size']):<13} {int(row['min_samples']):<13} {int(row['n_neighbors']):<13} {int(row['n_components'])}\")\n",
                "    print(\"-\" * 100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "save-best-models",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Retraining and saving best models per subject...\n",
                        "================================================================================\n",
                        "\n",
                        "CS (model: all-distilroberta-v1): Retraining with best params (coherence=0.7364)\n",
                        "  min_cluster_size=100, min_samples=10, n_neighbors=25, n_components=5\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-14 17:40:00,111 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics: 261\n",
                        "  ✓ Saved to tunning/hdbscan_v1/best_cs\n",
                        "\n",
                        "MATH (model: BAAI/bge-base-en-v1.5): Retraining with best params (coherence=0.7229)\n",
                        "  min_cluster_size=150, min_samples=10, n_neighbors=15, n_components=5\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-14 17:42:16,665 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics: 150\n",
                        "  ✓ Saved to tunning/hdbscan_v1/best_math\n",
                        "\n",
                        "PHYSICS (model: all-distilroberta-v1): Retraining with best params (coherence=0.7466)\n",
                        "  min_cluster_size=100, min_samples=5, n_neighbors=25, n_components=10\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-14 17:44:58,262 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics: 232\n",
                        "  ✓ Saved to tunning/hdbscan_v1/best_physics\n",
                        "\n",
                        "================================================================================\n",
                        "All best models saved!\n",
                        "================================================================================\n"
                    ]
                }
            ],
            "source": [
                "results_df = pd.read_csv(csv_path)\n",
                "\n",
                "print(\"Retraining and saving best models per subject...\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "for subject in LIST_SUBJECT:\n",
                "    if subject not in all_embeddings:\n",
                "        print(f\"\\nSkipping {subject} (no embeddings)\")\n",
                "        continue\n",
                "\n",
                "    subject_df = results_df[results_df['subject'] == subject].dropna(subset=['coherence'])\n",
                "    if len(subject_df) == 0:\n",
                "        continue\n",
                "\n",
                "    best = subject_df.loc[subject_df['coherence'].idxmax()]\n",
                "    model_info = SUBJECT_MODEL[subject]\n",
                "    texts = all_data[subject][\"text\"].fillna(\"\").tolist()\n",
                "    embeddings = all_embeddings[subject]\n",
                "\n",
                "    print(f\"\\n{subject.upper()} (model: {model_info['name']}): Retraining with best params (coherence={best['coherence']:.4f})\")\n",
                "    print(f\"  min_cluster_size={int(best['min_cluster_size'])}, min_samples={int(best['min_samples'])}, \"\n",
                "          f\"n_neighbors={int(best['n_neighbors'])}, n_components={int(best['n_components'])}\")\n",
                "\n",
                "    umap_model = UMAP(\n",
                "        n_neighbors=int(best[\"n_neighbors\"]),\n",
                "        n_components=int(best[\"n_components\"]),\n",
                "        metric=\"cosine\",\n",
                "        random_state=42,\n",
                "        min_dist=0.0,\n",
                "        verbose=False\n",
                "    )\n",
                "\n",
                "    hdbscan_model = HDBSCAN(\n",
                "        min_cluster_size=int(best[\"min_cluster_size\"]),\n",
                "        min_samples=int(best[\"min_samples\"]),\n",
                "        metric=\"euclidean\",\n",
                "        cluster_selection_method=\"eom\",\n",
                "        prediction_data=True\n",
                "    )\n",
                "\n",
                "    topic_model = BERTopic(\n",
                "        umap_model=umap_model,\n",
                "        hdbscan_model=hdbscan_model,\n",
                "        calculate_probabilities=False,\n",
                "        verbose=False\n",
                "    )\n",
                "\n",
                "    topics, _ = topic_model.fit_transform(texts, embeddings=embeddings)\n",
                "    n_topics = len(topic_model.get_topic_info()) - 1\n",
                "    print(f\"  Topics: {n_topics}\")\n",
                "\n",
                "    model_save_path = str(TUNING_DIR / f\"best_{subject}\")\n",
                "    topic_model.save(model_save_path)\n",
                "    print(f\"  ✓ Saved to {model_save_path}\")\n",
                "\n",
                "    del topic_model\n",
                "    gc.collect()\n",
                "\n",
                "print(f\"\\n{'='*80}\")\n",
                "print(\"All best models saved!\")\n",
                "print(f\"{'='*80}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
