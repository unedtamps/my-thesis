{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro-markdown",
            "metadata": {},
            "source": [
                "# BERTopic Hyperparameter Tuning (Grid Search)\n",
                "\n",
                "This notebook performs grid search over UMAP and HDBSCAN hyperparameters\n",
                "using **all-distilroberta-v1** embeddings across all subjects.\n",
                "Results and best models are saved to the `tunning/` directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import gc\n",
                "import itertools\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from typing import List, Optional, Tuple\n",
                "from tqdm import tqdm\n",
                "import warnings\n",
                "\n",
                "from umap import UMAP\n",
                "from hdbscan import HDBSCAN\n",
                "from bertopic import BERTopic\n",
                "from gensim.utils import simple_preprocess\n",
                "from gensim.corpora import Dictionary\n",
                "from gensim.models import CoherenceModel\n",
                "\n",
                "pd.set_option('display.max_colwidth', None)\n",
                "warnings.filterwarnings(\"ignore\", category=SyntaxWarning)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "config",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model: all-distilroberta-v1\n",
                        "Grid search: 54 combinations per subject\n",
                        "Total trials: 162\n",
                        "Results will be saved to: tunning/hdbscan_all_distilroberta_v1_v1\n"
                    ]
                }
            ],
            "source": [
                "VERSION = \"v1\"\n",
                "MODEL_NAME = \"all-distilroberta-v1\"\n",
                "MODEL_SAFE_NAME = \"all_distilroberta_v1\"\n",
                "EMBEDDING_DIM = 768\n",
                "LIST_SUBJECT = [\"cs\", \"math\", \"physics\"]\n",
                "\n",
                "PARAM_GRID = {\n",
                "    \"min_cluster_size\": [100, 150, 200],\n",
                "    \"min_samples\": [5, 10, 20],\n",
                "    \"n_neighbors\": [10, 15, 25],\n",
                "    \"n_components\": [5, 10],\n",
                "}\n",
                "\n",
                "BASE_DIR = Path(\"../../dataset\")\n",
                "EMBEDDING_DIR = Path(\"./embedding\")\n",
                "TUNING_DIR = Path(f\"./tunning/hdbscan_{MODEL_SAFE_NAME}_{VERSION}\")\n",
                "\n",
                "TUNING_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "total_combos = 1\n",
                "for values in PARAM_GRID.values():\n",
                "    total_combos *= len(values)\n",
                "print(f\"Model: {MODEL_NAME}\")\n",
                "print(f\"Grid search: {total_combos} combinations per subject\")\n",
                "print(f\"Total trials: {total_combos * len(LIST_SUBJECT)}\")\n",
                "print(f\"Results will be saved to: {TUNING_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "helper-functions",
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_dataset(subject: str) -> pd.DataFrame:\n",
                "    file_path = BASE_DIR / subject / \"emb\" / f\"{VERSION}.csv\"\n",
                "    if not file_path.exists():\n",
                "        print(f\"File not found: {file_path}\")\n",
                "        return None\n",
                "    return pd.read_csv(file_path)\n",
                "\n",
                "\n",
                "def load_mmap_embeddings(\n",
                "    mmap_path: str,\n",
                "    num_documents: int,\n",
                "    embedding_dim: int,\n",
                "    dtype: str = \"float32\"\n",
                ") -> Optional[np.memmap]:\n",
                "    try:\n",
                "        return np.memmap(\n",
                "            mmap_path, dtype=dtype, mode=\"r\",\n",
                "            shape=(num_documents, embedding_dim)\n",
                "        )\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading embeddings: {e}\")\n",
                "        return None\n",
                "\n",
                "\n",
                "def tokenize_for_coherence(text: str) -> List[str]:\n",
                "    return [token for token in simple_preprocess(str(text), deacc=True)]\n",
                "\n",
                "\n",
                "def calculate_coherence(\n",
                "    topic_model: BERTopic,\n",
                "    texts_tokenized: List[List[str]],\n",
                "    dictionary: Dictionary,\n",
                "    top_n: int = 10\n",
                ") -> float:\n",
                "    topics_list = []\n",
                "    for topic_id in topic_model.get_topics().keys():\n",
                "        if topic_id == -1:\n",
                "            continue\n",
                "        topic_words = [word for word, _ in topic_model.get_topic(topic_id)[:top_n]]\n",
                "        topics_list.append(topic_words)\n",
                "\n",
                "    if not topics_list:\n",
                "        return 0.0\n",
                "\n",
                "    cm = CoherenceModel(\n",
                "        topics=topics_list,\n",
                "        texts=texts_tokenized,\n",
                "        dictionary=dictionary,\n",
                "        coherence='c_v',\n",
                "        processes=1\n",
                "    )\n",
                "    return cm.get_coherence()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "load-data",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "cs: 165,756 documents loaded\n",
                        "  Tokenizing for coherence...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  cs: 100%|██████████| 165756/165756 [00:27<00:00, 5998.23it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Embeddings loaded: (165756, 768)\n",
                        "math: 126,192 documents loaded\n",
                        "  Tokenizing for coherence...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  math: 100%|██████████| 126192/126192 [00:13<00:00, 9157.70it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Embeddings loaded: (126192, 768)\n",
                        "physics: 146,311 documents loaded\n",
                        "  Tokenizing for coherence...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  physics: 100%|██████████| 146311/146311 [00:22<00:00, 6385.38it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Embeddings loaded: (146311, 768)\n",
                        "\n",
                        "Subjects ready: ['cs', 'math', 'physics']\n"
                    ]
                }
            ],
            "source": [
                "all_data = {}\n",
                "all_texts_tokenized = {}\n",
                "all_dictionaries = {}\n",
                "all_embeddings = {}\n",
                "\n",
                "for subject in LIST_SUBJECT:\n",
                "    df = load_dataset(subject)\n",
                "    if df is None:\n",
                "        continue\n",
                "\n",
                "    all_data[subject] = df\n",
                "    texts = df[\"text\"].fillna(\"\").tolist()\n",
                "    print(f\"{subject}: {len(df):,} documents loaded\")\n",
                "\n",
                "    print(f\"  Tokenizing for coherence...\")\n",
                "    texts_tokenized = [tokenize_for_coherence(t) for t in tqdm(texts, desc=f\"  {subject}\")]\n",
                "    all_texts_tokenized[subject] = texts_tokenized\n",
                "    all_dictionaries[subject] = Dictionary(texts_tokenized)\n",
                "\n",
                "    emb_path = str(EMBEDDING_DIR / subject / f\"{MODEL_SAFE_NAME}_{VERSION}.mmap\")\n",
                "    embeddings = load_mmap_embeddings(emb_path, len(texts), EMBEDDING_DIM)\n",
                "    if embeddings is not None:\n",
                "        all_embeddings[subject] = embeddings\n",
                "        print(f\"  Embeddings loaded: {embeddings.shape}\")\n",
                "    else:\n",
                "        print(f\"  ⚠ Failed to load embeddings\")\n",
                "\n",
                "print(f\"\\nSubjects ready: {list(all_data.keys())}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "grid-search",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 54 existing results from tunning/hdbscan_all_distilroberta_v1_v1/coherence_results.csv\n",
                        "\n",
                        "================================================================================\n",
                        "Subject: CS (165,756 documents)\n",
                        "================================================================================\n",
                        "Combinations to run: 0 / 54\n",
                        "\n",
                        "================================================================================\n",
                        "Subject: MATH (126,192 documents)\n",
                        "================================================================================\n",
                        "Combinations to run: 54 / 54\n",
                        "\n",
                        "[1/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 193 | Coherence: 0.7006 | Outliers: 38.41%\n",
                        "\n",
                        "[2/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 215 | Coherence: 0.7093 | Outliers: 41.61%\n",
                        "\n",
                        "[3/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 208 | Coherence: 0.7108 | Outliers: 45.37%\n",
                        "\n",
                        "[4/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 206 | Coherence: 0.7077 | Outliers: 45.16%\n",
                        "\n",
                        "[5/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 188 | Coherence: 0.7040 | Outliers: 48.95%\n",
                        "\n",
                        "[6/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 194 | Coherence: 0.7023 | Outliers: 52.45%\n",
                        "\n",
                        "[7/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 194 | Coherence: 0.7025 | Outliers: 39.11%\n",
                        "\n",
                        "[8/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 202 | Coherence: 0.7092 | Outliers: 40.39%\n",
                        "\n",
                        "[9/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 195 | Coherence: 0.6985 | Outliers: 45.00%\n",
                        "\n",
                        "[10/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 190 | Coherence: 0.7024 | Outliers: 43.19%\n",
                        "\n",
                        "[11/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 179 | Coherence: 0.7003 | Outliers: 50.54%\n",
                        "\n",
                        "[12/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 177 | Coherence: 0.6954 | Outliers: 49.50%\n",
                        "\n",
                        "[13/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 190 | Coherence: 0.7071 | Outliers: 43.41%\n",
                        "\n",
                        "[14/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 184 | Coherence: 0.7072 | Outliers: 41.73%\n",
                        "\n",
                        "[15/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 182 | Coherence: 0.7076 | Outliers: 48.71%\n",
                        "\n",
                        "[16/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 174 | Coherence: 0.7051 | Outliers: 46.25%\n",
                        "\n",
                        "[17/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 167 | Coherence: 0.7063 | Outliers: 51.82%\n",
                        "\n",
                        "[18/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 167 | Coherence: 0.6997 | Outliers: 51.13%\n",
                        "\n",
                        "[19/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 135 | Coherence: 0.7012 | Outliers: 39.14%\n",
                        "\n",
                        "[20/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 134 | Coherence: 0.7014 | Outliers: 39.78%\n",
                        "\n",
                        "[21/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 139 | Coherence: 0.7053 | Outliers: 45.05%\n",
                        "\n",
                        "[22/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 133 | Coherence: 0.7014 | Outliers: 45.04%\n",
                        "\n",
                        "[23/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 126 | Coherence: 0.7046 | Outliers: 50.23%\n",
                        "\n",
                        "[24/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 127 | Coherence: 0.6908 | Outliers: 49.52%\n",
                        "\n",
                        "[25/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 137 | Coherence: 0.7052 | Outliers: 39.85%\n",
                        "\n",
                        "[26/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 132 | Coherence: 0.7014 | Outliers: 40.46%\n",
                        "\n",
                        "[27/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 135 | Coherence: 0.7018 | Outliers: 45.33%\n",
                        "\n",
                        "[28/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 124 | Coherence: 0.6955 | Outliers: 41.58%\n",
                        "\n",
                        "[29/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 124 | Coherence: 0.7009 | Outliers: 50.08%\n",
                        "\n",
                        "[30/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 122 | Coherence: 0.6951 | Outliers: 50.13%\n",
                        "\n",
                        "[31/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 135 | Coherence: 0.7095 | Outliers: 43.23%\n",
                        "\n",
                        "[32/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 123 | Coherence: 0.7057 | Outliers: 42.40%\n",
                        "\n",
                        "[33/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 130 | Coherence: 0.7085 | Outliers: 49.69%\n",
                        "\n",
                        "[34/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 118 | Coherence: 0.6996 | Outliers: 46.44%\n",
                        "\n",
                        "[35/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 116 | Coherence: 0.7000 | Outliers: 51.20%\n",
                        "\n",
                        "[36/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 113 | Coherence: 0.6963 | Outliers: 49.91%\n",
                        "\n",
                        "[37/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 107 | Coherence: 0.6933 | Outliers: 39.31%\n",
                        "\n",
                        "[38/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 116 | Coherence: 0.7093 | Outliers: 41.35%\n",
                        "\n",
                        "[39/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 111 | Coherence: 0.6992 | Outliers: 45.87%\n",
                        "\n",
                        "[40/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 101 | Coherence: 0.6962 | Outliers: 44.98%\n",
                        "\n",
                        "[41/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 102 | Coherence: 0.7029 | Outliers: 52.33%\n",
                        "\n",
                        "[42/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 97 | Coherence: 0.6990 | Outliers: 50.65%\n",
                        "\n",
                        "[43/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 110 | Coherence: 0.7059 | Outliers: 40.87%\n",
                        "\n",
                        "[44/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 110 | Coherence: 0.7012 | Outliers: 39.81%\n",
                        "\n",
                        "[45/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 104 | Coherence: 0.6969 | Outliers: 42.02%\n",
                        "\n",
                        "[46/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 104 | Coherence: 0.6944 | Outliers: 41.03%\n",
                        "\n",
                        "[47/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 97 | Coherence: 0.6894 | Outliers: 48.57%\n",
                        "\n",
                        "[48/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 89 | Coherence: 0.6879 | Outliers: 49.53%\n",
                        "\n",
                        "[49/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 103 | Coherence: 0.6946 | Outliers: 40.31%\n",
                        "\n",
                        "[50/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 103 | Coherence: 0.7011 | Outliers: 42.06%\n",
                        "\n",
                        "[51/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 95 | Coherence: 0.7028 | Outliers: 45.72%\n",
                        "\n",
                        "[52/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 91 | Coherence: 0.6876 | Outliers: 43.88%\n",
                        "\n",
                        "[53/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 88 | Coherence: 0.6894 | Outliers: 47.42%\n",
                        "\n",
                        "[54/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 94 | Coherence: 0.6919 | Outliers: 49.23%\n",
                        "\n",
                        "================================================================================\n",
                        "Subject: PHYSICS (146,311 documents)\n",
                        "================================================================================\n",
                        "Combinations to run: 54 / 54\n",
                        "\n",
                        "[1/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 267 | Coherence: 0.7460 | Outliers: 40.42%\n",
                        "\n",
                        "[2/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 251 | Coherence: 0.7436 | Outliers: 39.81%\n",
                        "\n",
                        "[3/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 225 | Coherence: 0.7392 | Outliers: 40.72%\n",
                        "\n",
                        "[4/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 231 | Coherence: 0.7378 | Outliers: 40.55%\n",
                        "\n",
                        "[5/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 229 | Coherence: 0.7417 | Outliers: 44.73%\n",
                        "\n",
                        "[6/54] {'min_cluster_size': 100, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 232 | Coherence: 0.7466 | Outliers: 45.30%\n",
                        "\n",
                        "[7/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 245 | Coherence: 0.7400 | Outliers: 40.58%\n",
                        "\n",
                        "[8/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 242 | Coherence: 0.7429 | Outliers: 39.94%\n",
                        "\n",
                        "[9/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 215 | Coherence: 0.7426 | Outliers: 41.47%\n",
                        "\n",
                        "[10/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 214 | Coherence: 0.7399 | Outliers: 40.13%\n",
                        "\n",
                        "[11/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 214 | Coherence: 0.7432 | Outliers: 43.61%\n",
                        "\n",
                        "[12/54] {'min_cluster_size': 100, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 208 | Coherence: 0.7370 | Outliers: 43.84%\n",
                        "\n",
                        "[13/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 231 | Coherence: 0.7397 | Outliers: 42.52%\n",
                        "\n",
                        "[14/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 233 | Coherence: 0.7438 | Outliers: 41.74%\n",
                        "\n",
                        "[15/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 210 | Coherence: 0.7442 | Outliers: 42.96%\n",
                        "\n",
                        "[16/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 204 | Coherence: 0.7363 | Outliers: 41.22%\n",
                        "\n",
                        "[17/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 198 | Coherence: 0.7438 | Outliers: 45.06%\n",
                        "\n",
                        "[18/54] {'min_cluster_size': 100, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 195 | Coherence: 0.7453 | Outliers: 44.85%\n",
                        "\n",
                        "[19/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 196 | Coherence: 0.7433 | Outliers: 40.34%\n",
                        "\n",
                        "[20/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 179 | Coherence: 0.7414 | Outliers: 39.29%\n",
                        "\n",
                        "[21/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 171 | Coherence: 0.7392 | Outliers: 40.63%\n",
                        "\n",
                        "[22/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 172 | Coherence: 0.7373 | Outliers: 39.79%\n",
                        "\n",
                        "[23/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 169 | Coherence: 0.7456 | Outliers: 44.26%\n",
                        "\n",
                        "[24/54] {'min_cluster_size': 150, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 157 | Coherence: 0.7422 | Outliers: 44.31%\n",
                        "\n",
                        "[25/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 179 | Coherence: 0.7379 | Outliers: 40.10%\n",
                        "\n",
                        "[26/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 177 | Coherence: 0.7458 | Outliers: 39.88%\n",
                        "\n",
                        "[27/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 166 | Coherence: 0.7387 | Outliers: 41.90%\n",
                        "\n",
                        "[28/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 165 | Coherence: 0.7404 | Outliers: 40.37%\n",
                        "\n",
                        "[29/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 165 | Coherence: 0.7442 | Outliers: 43.95%\n",
                        "\n",
                        "[30/54] {'min_cluster_size': 150, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 154 | Coherence: 0.7425 | Outliers: 42.25%\n",
                        "\n",
                        "[31/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 167 | Coherence: 0.7353 | Outliers: 40.13%\n",
                        "\n",
                        "[32/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 176 | Coherence: 0.7445 | Outliers: 40.86%\n",
                        "\n",
                        "[33/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 164 | Coherence: 0.7419 | Outliers: 42.53%\n",
                        "\n",
                        "[34/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 165 | Coherence: 0.7416 | Outliers: 41.84%\n",
                        "\n",
                        "[35/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 160 | Coherence: 0.7421 | Outliers: 44.78%\n",
                        "\n",
                        "[36/54] {'min_cluster_size': 150, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 143 | Coherence: 0.7384 | Outliers: 43.19%\n",
                        "\n",
                        "[37/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 147 | Coherence: 0.7350 | Outliers: 39.13%\n",
                        "\n",
                        "[38/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 143 | Coherence: 0.7287 | Outliers: 38.67%\n",
                        "\n",
                        "[39/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 133 | Coherence: 0.7336 | Outliers: 41.00%\n",
                        "\n",
                        "[40/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 119 | Coherence: 0.7159 | Outliers: 35.44%\n",
                        "\n",
                        "[41/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 128 | Coherence: 0.7315 | Outliers: 43.02%\n",
                        "\n",
                        "[42/54] {'min_cluster_size': 200, 'min_samples': 5, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 122 | Coherence: 0.7271 | Outliers: 43.58%\n",
                        "\n",
                        "[43/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 142 | Coherence: 0.7316 | Outliers: 38.58%\n",
                        "\n",
                        "[44/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 147 | Coherence: 0.7368 | Outliers: 40.01%\n",
                        "\n",
                        "[45/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 122 | Coherence: 0.7201 | Outliers: 40.18%\n",
                        "\n",
                        "[46/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 126 | Coherence: 0.7280 | Outliers: 40.26%\n",
                        "\n",
                        "[47/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 124 | Coherence: 0.7324 | Outliers: 42.07%\n",
                        "\n",
                        "[48/54] {'min_cluster_size': 200, 'min_samples': 10, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 118 | Coherence: 0.7197 | Outliers: 40.13%\n",
                        "\n",
                        "[49/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 5}\n",
                        "  Topics: 134 | Coherence: 0.7158 | Outliers: 36.74%\n",
                        "\n",
                        "[50/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 10, 'n_components': 10}\n",
                        "  Topics: 139 | Coherence: 0.7255 | Outliers: 39.34%\n",
                        "\n",
                        "[51/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 5}\n",
                        "  Topics: 118 | Coherence: 0.7182 | Outliers: 39.10%\n",
                        "\n",
                        "[52/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 15, 'n_components': 10}\n",
                        "  Topics: 125 | Coherence: 0.7232 | Outliers: 38.94%\n",
                        "\n",
                        "[53/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 5}\n",
                        "  Topics: 112 | Coherence: 0.7138 | Outliers: 40.65%\n",
                        "\n",
                        "[54/54] {'min_cluster_size': 200, 'min_samples': 20, 'n_neighbors': 25, 'n_components': 10}\n",
                        "  Topics: 113 | Coherence: 0.7205 | Outliers: 40.87%\n",
                        "\n",
                        "================================================================================\n",
                        "Grid search complete! Results saved to: tunning/hdbscan_all_distilroberta_v1_v1/coherence_results.csv\n",
                        "Total trials: 162\n",
                        "================================================================================\n"
                    ]
                }
            ],
            "source": [
                "param_keys = list(PARAM_GRID.keys())\n",
                "param_values = list(PARAM_GRID.values())\n",
                "all_combos = list(itertools.product(*param_values))\n",
                "\n",
                "results = []\n",
                "csv_path = TUNING_DIR / \"coherence_results.csv\"\n",
                "\n",
                "existing_results = set()\n",
                "if csv_path.exists():\n",
                "    existing_df = pd.read_csv(csv_path)\n",
                "    for _, row in existing_df.iterrows():\n",
                "        key = (row['subject'], row['min_cluster_size'], row['min_samples'],\n",
                "               row['n_neighbors'], row['n_components'])\n",
                "        existing_results.add(key)\n",
                "    results = existing_df.to_dict('records')\n",
                "    print(f\"Loaded {len(results)} existing results from {csv_path}\")\n",
                "\n",
                "for subject in LIST_SUBJECT:\n",
                "    if subject not in all_embeddings:\n",
                "        print(f\"\\nSkipping {subject} (no embeddings)\")\n",
                "        continue\n",
                "\n",
                "    texts = all_data[subject][\"text\"].fillna(\"\").tolist()\n",
                "    embeddings = all_embeddings[subject]\n",
                "    texts_tokenized = all_texts_tokenized[subject]\n",
                "    dictionary = all_dictionaries[subject]\n",
                "\n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(f\"Subject: {subject.upper()} ({len(texts):,} documents)\")\n",
                "    print(f\"{'='*80}\")\n",
                "\n",
                "    subject_combos = [\n",
                "        combo for combo in all_combos\n",
                "        if (subject, *combo) not in existing_results\n",
                "    ]\n",
                "    print(f\"Combinations to run: {len(subject_combos)} / {len(all_combos)}\")\n",
                "\n",
                "    for i, combo in enumerate(subject_combos, 1):\n",
                "        params = dict(zip(param_keys, combo))\n",
                "\n",
                "        print(f\"\\n[{i}/{len(subject_combos)}] {params}\")\n",
                "\n",
                "        try:\n",
                "            umap_model = UMAP(\n",
                "                n_neighbors=params[\"n_neighbors\"],\n",
                "                n_components=params[\"n_components\"],\n",
                "                metric=\"cosine\",\n",
                "                random_state=42,\n",
                "                min_dist=0.0,\n",
                "                verbose=False\n",
                "            )\n",
                "\n",
                "            hdbscan_model = HDBSCAN(\n",
                "                min_cluster_size=params[\"min_cluster_size\"],\n",
                "                min_samples=params[\"min_samples\"],\n",
                "                metric=\"euclidean\",\n",
                "                cluster_selection_method=\"eom\",\n",
                "                prediction_data=True\n",
                "            )\n",
                "\n",
                "            topic_model = BERTopic(\n",
                "                umap_model=umap_model,\n",
                "                hdbscan_model=hdbscan_model,\n",
                "                calculate_probabilities=False,\n",
                "                verbose=False\n",
                "            )\n",
                "\n",
                "            topics, _ = topic_model.fit_transform(texts, embeddings=embeddings)\n",
                "            n_topics = len(topic_model.get_topic_info()) - 1\n",
                "            outlier_count = sum(1 for t in topics if t == -1)\n",
                "            outlier_ratio = outlier_count / len(topics)\n",
                "\n",
                "            coherence = calculate_coherence(topic_model, texts_tokenized, dictionary)\n",
                "\n",
                "            print(f\"  Topics: {n_topics} | Coherence: {coherence:.4f} | Outliers: {outlier_ratio:.2%}\")\n",
                "\n",
                "            results.append({\n",
                "                \"subject\": subject,\n",
                "                \"model\": MODEL_NAME,\n",
                "                \"min_cluster_size\": params[\"min_cluster_size\"],\n",
                "                \"min_samples\": params[\"min_samples\"],\n",
                "                \"n_neighbors\": params[\"n_neighbors\"],\n",
                "                \"n_components\": params[\"n_components\"],\n",
                "                \"n_topics\": n_topics,\n",
                "                \"coherence\": coherence,\n",
                "                \"outlier_ratio\": outlier_ratio\n",
                "            })\n",
                "\n",
                "            del topic_model\n",
                "            gc.collect()\n",
                "\n",
                "        except Exception as e:\n",
                "            print(f\"  ✗ Error: {e}\")\n",
                "            results.append({\n",
                "                \"subject\": subject,\n",
                "                \"model\": MODEL_NAME,\n",
                "                \"min_cluster_size\": params[\"min_cluster_size\"],\n",
                "                \"min_samples\": params[\"min_samples\"],\n",
                "                \"n_neighbors\": params[\"n_neighbors\"],\n",
                "                \"n_components\": params[\"n_components\"],\n",
                "                \"n_topics\": None,\n",
                "                \"coherence\": None,\n",
                "                \"outlier_ratio\": None\n",
                "            })\n",
                "\n",
                "        results_df = pd.DataFrame(results)\n",
                "        results_df.to_csv(csv_path, index=False)\n",
                "\n",
                "print(f\"\\n{'='*80}\")\n",
                "print(f\"Grid search complete! Results saved to: {csv_path}\")\n",
                "print(f\"Total trials: {len(results)}\")\n",
                "print(f\"{'='*80}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "show-results",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Best Hyperparameters per Subject\n",
                        "====================================================================================================\n",
                        "\n",
                        "CS:\n",
                        "  Best Coherence: 0.7364\n",
                        "  Topics: 261\n",
                        "  Outlier Ratio: 40.55%\n",
                        "  Parameters:\n",
                        "    min_cluster_size = 100\n",
                        "    min_samples      = 10\n",
                        "    n_neighbors      = 25\n",
                        "    n_components     = 5\n",
                        "\n",
                        "  Top 5 Configurations:\n",
                        "  Rank  Coherence   Topics   Outliers   min_cluster   min_samples   n_neighbors   n_components\n",
                        "  ------------------------------------------------------------------------------------------\n",
                        "  #1    0.7364     261      40.55%     100           10            25            5\n",
                        "  #2    0.7352     202      36.52%     150           5             15            5\n",
                        "  #3    0.7345     243      41.23%     100           20            25            5\n",
                        "  #4    0.7340     256      37.30%     100           5             25            10\n",
                        "  #5    0.7332     273      37.47%     100           10            15            5\n",
                        "----------------------------------------------------------------------------------------------------\n",
                        "\n",
                        "MATH:\n",
                        "  Best Coherence: 0.7108\n",
                        "  Topics: 208\n",
                        "  Outlier Ratio: 45.37%\n",
                        "  Parameters:\n",
                        "    min_cluster_size = 100\n",
                        "    min_samples      = 5\n",
                        "    n_neighbors      = 15\n",
                        "    n_components     = 5\n",
                        "\n",
                        "  Top 5 Configurations:\n",
                        "  Rank  Coherence   Topics   Outliers   min_cluster   min_samples   n_neighbors   n_components\n",
                        "  ------------------------------------------------------------------------------------------\n",
                        "  #1    0.7108     208      45.37%     100           5             15            5\n",
                        "  #2    0.7095     135      43.23%     150           20            10            5\n",
                        "  #3    0.7093     116      41.35%     200           5             10            10\n",
                        "  #4    0.7093     215      41.61%     100           5             10            10\n",
                        "  #5    0.7092     202      40.39%     100           10            10            10\n",
                        "----------------------------------------------------------------------------------------------------\n",
                        "\n",
                        "PHYSICS:\n",
                        "  Best Coherence: 0.7466\n",
                        "  Topics: 232\n",
                        "  Outlier Ratio: 45.30%\n",
                        "  Parameters:\n",
                        "    min_cluster_size = 100\n",
                        "    min_samples      = 5\n",
                        "    n_neighbors      = 25\n",
                        "    n_components     = 10\n",
                        "\n",
                        "  Top 5 Configurations:\n",
                        "  Rank  Coherence   Topics   Outliers   min_cluster   min_samples   n_neighbors   n_components\n",
                        "  ------------------------------------------------------------------------------------------\n",
                        "  #1    0.7466     232      45.30%     100           5             25            10\n",
                        "  #2    0.7460     267      40.42%     100           5             10            5\n",
                        "  #3    0.7458     177      39.88%     150           10            10            10\n",
                        "  #4    0.7456     169      44.26%     150           5             25            5\n",
                        "  #5    0.7453     195      44.85%     100           20            25            10\n",
                        "----------------------------------------------------------------------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "results_df = pd.read_csv(csv_path)\n",
                "\n",
                "print(\"\\nBest Hyperparameters per Subject\")\n",
                "print(\"=\" * 100)\n",
                "\n",
                "for subject in LIST_SUBJECT:\n",
                "    subject_df = results_df[results_df['subject'] == subject].dropna(subset=['coherence'])\n",
                "    if len(subject_df) == 0:\n",
                "        continue\n",
                "\n",
                "    best = subject_df.loc[subject_df['coherence'].idxmax()]\n",
                "\n",
                "    print(f\"\\n{subject.upper()}:\")\n",
                "    print(f\"  Best Coherence: {best['coherence']:.4f}\")\n",
                "    print(f\"  Topics: {int(best['n_topics'])}\")\n",
                "    print(f\"  Outlier Ratio: {best['outlier_ratio']:.2%}\")\n",
                "    print(f\"  Parameters:\")\n",
                "    print(f\"    min_cluster_size = {int(best['min_cluster_size'])}\")\n",
                "    print(f\"    min_samples      = {int(best['min_samples'])}\")\n",
                "    print(f\"    n_neighbors      = {int(best['n_neighbors'])}\")\n",
                "    print(f\"    n_components     = {int(best['n_components'])}\")\n",
                "\n",
                "    top5 = subject_df.nlargest(5, 'coherence')\n",
                "    print(f\"\\n  Top 5 Configurations:\")\n",
                "    print(f\"  {'Rank':<5} {'Coherence':<11} {'Topics':<8} {'Outliers':<10} {'min_cluster':<13} {'min_samples':<13} {'n_neighbors':<13} {'n_components'}\")\n",
                "    print(f\"  {'-'*90}\")\n",
                "    for rank, (_, row) in enumerate(top5.iterrows(), 1):\n",
                "        print(f\"  #{rank:<4} {row['coherence']:.4f}     {int(row['n_topics']):<8} {row['outlier_ratio']:.2%}     {int(row['min_cluster_size']):<13} {int(row['min_samples']):<13} {int(row['n_neighbors']):<13} {int(row['n_components'])}\")\n",
                "    print(\"-\" * 100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "save-best-models",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Retraining and saving best models per subject...\n",
                        "================================================================================\n",
                        "\n",
                        "CS: Retraining with best params (coherence=0.7364)\n",
                        "  min_cluster_size=100, min_samples=10, n_neighbors=25, n_components=5\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-12 16:20:40,564 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics: 261\n",
                        "  ✓ Saved to tunning/hdbscan_all_distilroberta_v1_v1/best_cs\n",
                        "\n",
                        "MATH: Retraining with best params (coherence=0.7108)\n",
                        "  min_cluster_size=100, min_samples=5, n_neighbors=15, n_components=5\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-12 16:22:35,322 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics: 208\n",
                        "  ✓ Saved to tunning/hdbscan_all_distilroberta_v1_v1/best_math\n",
                        "\n",
                        "PHYSICS: Retraining with best params (coherence=0.7466)\n",
                        "  min_cluster_size=100, min_samples=5, n_neighbors=25, n_components=10\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2026-02-12 16:25:18,685 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Topics: 232\n",
                        "  ✓ Saved to tunning/hdbscan_all_distilroberta_v1_v1/best_physics\n",
                        "\n",
                        "================================================================================\n",
                        "All best models saved!\n",
                        "================================================================================\n"
                    ]
                }
            ],
            "source": [
                "results_df = pd.read_csv(csv_path)\n",
                "\n",
                "print(\"Retraining and saving best models per subject...\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "for subject in LIST_SUBJECT:\n",
                "    if subject not in all_embeddings:\n",
                "        print(f\"\\nSkipping {subject} (no embeddings)\")\n",
                "        continue\n",
                "\n",
                "    subject_df = results_df[results_df['subject'] == subject].dropna(subset=['coherence'])\n",
                "    if len(subject_df) == 0:\n",
                "        continue\n",
                "\n",
                "    best = subject_df.loc[subject_df['coherence'].idxmax()]\n",
                "    texts = all_data[subject][\"text\"].fillna(\"\").tolist()\n",
                "    embeddings = all_embeddings[subject]\n",
                "\n",
                "    print(f\"\\n{subject.upper()}: Retraining with best params (coherence={best['coherence']:.4f})\")\n",
                "    print(f\"  min_cluster_size={int(best['min_cluster_size'])}, min_samples={int(best['min_samples'])}, \"\n",
                "          f\"n_neighbors={int(best['n_neighbors'])}, n_components={int(best['n_components'])}\")\n",
                "\n",
                "    umap_model = UMAP(\n",
                "        n_neighbors=int(best[\"n_neighbors\"]),\n",
                "        n_components=int(best[\"n_components\"]),\n",
                "        metric=\"cosine\",\n",
                "        random_state=42,\n",
                "        min_dist=0.0,\n",
                "        verbose=False\n",
                "    )\n",
                "\n",
                "    hdbscan_model = HDBSCAN(\n",
                "        min_cluster_size=int(best[\"min_cluster_size\"]),\n",
                "        min_samples=int(best[\"min_samples\"]),\n",
                "        metric=\"euclidean\",\n",
                "        cluster_selection_method=\"eom\",\n",
                "        prediction_data=True\n",
                "    )\n",
                "\n",
                "    topic_model = BERTopic(\n",
                "        umap_model=umap_model,\n",
                "        hdbscan_model=hdbscan_model,\n",
                "        calculate_probabilities=False,\n",
                "        verbose=False\n",
                "    )\n",
                "\n",
                "    topics, _ = topic_model.fit_transform(texts, embeddings=embeddings)\n",
                "    n_topics = len(topic_model.get_topic_info()) - 1\n",
                "    print(f\"  Topics: {n_topics}\")\n",
                "\n",
                "    model_save_path = str(TUNING_DIR / f\"best_{subject}\")\n",
                "    topic_model.save(model_save_path)\n",
                "    print(f\"  ✓ Saved to {model_save_path}\")\n",
                "\n",
                "    del topic_model\n",
                "    gc.collect()\n",
                "\n",
                "print(f\"\\n{'='*80}\")\n",
                "print(\"All best models saved!\")\n",
                "print(f\"{'='*80}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
