{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "intro-markdown",
            "metadata": {},
            "source": [
                "# Embedding Generation for BERTopic\n",
                "\n",
                "This notebook generates embeddings for each subject dataset (cs, eess, math, physics, stat) using multiple transformer models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch\n",
                "from pathlib import Path\n",
                "from typing import List\n",
                "from tqdm import tqdm\n",
                "from sentence_transformers import SentenceTransformer\n",
                "from transformers import AutoTokenizer, AutoModel\n",
                "from peft import PeftModel"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "config",
            "metadata": {},
            "outputs": [],
            "source": [
                "LIST_SUBJECT = [\"cs\", \"math\", \"physics\"]\n",
                "\n",
                "TRANSFORMERS = [\n",
                "    \"sentence-transformers/all-MiniLM-L6-v2\",\n",
                "    \"all-distilroberta-v1\",\n",
                "    \"intfloat/e5-base-v2\",\n",
                "    \"all-mpnet-base-v2\", \n",
                "    \"BAAI/bge-base-en-v1.5\",\n",
                "    \"allenai/specter2\"\n",
                "]\n",
                "\n",
                "BASE_DIR = Path(\"../../dataset\")\n",
                "OUTPUT_DIR = Path(\"./embedding\")\n",
                "BATCH_SIZE = 128\n",
                "VERSION = \"v1\"\n",
                "\n",
                "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "6ac6f192",
            "metadata": {},
            "outputs": [],
            "source": [
                "from adapters import AutoAdapterModel\n",
                "\n",
                "def load_specter2():\n",
                "    tokenizer = AutoTokenizer.from_pretrained(\"allenai/specter2_base\")\n",
                "    model = AutoAdapterModel.from_pretrained(\"allenai/specter2_base\")\n",
                "    \n",
                "    # Load the proximity adapter\n",
                "    model.load_adapter(\"allenai/specter2\", source=\"hf\", set_active=True)\n",
                "    \n",
                "    model.eval()\n",
                "    if torch.cuda.is_available():\n",
                "        model = model.cuda()\n",
                "    return model, tokenizer\n",
                "\n",
                "\n",
                "def encode_specter2(texts: List[str], model, tokenizer, batch_size: int = 32) -> np.ndarray:\n",
                "    embeddings = []\n",
                "    device = next(model.parameters()).device\n",
                "    \n",
                "    for i in tqdm(range(0, len(texts), batch_size), desc=\"  Generating\"):\n",
                "        batch = texts[i:i + batch_size]\n",
                "        inputs = tokenizer(\n",
                "            batch,\n",
                "            padding=True,\n",
                "            truncation=True,\n",
                "            max_length=512,\n",
                "            return_tensors=\"pt\"\n",
                "        ).to(device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = model(**inputs)\n",
                "            batch_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
                "        \n",
                "        embeddings.append(batch_emb)\n",
                "    \n",
                "    return np.vstack(embeddings)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "helper-functions",
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_dataset(subject: str) -> pd.DataFrame:\n",
                "    file_path = BASE_DIR / subject / \"emb\" / f\"{VERSION}.csv\"\n",
                "    if not file_path.exists():\n",
                "        print(f\"File not found: {file_path}\")\n",
                "        return None\n",
                "    return pd.read_csv(file_path)\n",
                "\n",
                "\n",
                "def get_model_safe_name(model_name: str) -> str:\n",
                "    return model_name.replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
                "\n",
                "\n",
                "def generate_mmap_embeddings(\n",
                "    texts: List[str],\n",
                "    embedding_model: SentenceTransformer,\n",
                "    mmap_path: str,\n",
                "    batch_size: int = 1024\n",
                ") -> np.memmap:\n",
                "    N = len(texts)\n",
                "    emb_dim = embedding_model.get_sentence_embedding_dimension()\n",
                "    \n",
                "    if N == 0:\n",
                "        print(\"Error: Empty text list.\")\n",
                "        return None\n",
                "\n",
                "    print(f\"  Total documents: {N:,}\")\n",
                "    print(f\"  Embedding dimension: {emb_dim}\")\n",
                "    print(f\"  Batch size: {batch_size}\")\n",
                "    print(f\"  Output path: {mmap_path}\")\n",
                "\n",
                "    embs = np.memmap(\n",
                "        mmap_path, \n",
                "        dtype=\"float32\", \n",
                "        mode=\"w+\",\n",
                "        shape=(N, emb_dim)\n",
                "    )\n",
                "\n",
                "    for i in tqdm(range(0, N, batch_size), desc=\"  Generating\"):\n",
                "        batch_texts = texts[i:i + batch_size]\n",
                "        \n",
                "        batch_embeddings = embedding_model.encode(\n",
                "            batch_texts, \n",
                "            show_progress_bar=False, \n",
                "            convert_to_numpy=True\n",
                "        )\n",
                "        \n",
                "        embs[i:i + len(batch_texts)] = batch_embeddings\n",
                "\n",
                "    embs.flush()\n",
                "    \n",
                "    metadata = {\n",
                "        \"n_samples\": N,\n",
                "        \"emb_dim\": emb_dim,\n",
                "        \"dtype\": \"float32\"\n",
                "    }\n",
                "    np.save(mmap_path.replace(\".mmap\", \"_meta.npy\"), metadata)\n",
                "    \n",
                "    return embs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "load-data",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "cs: 165,756 documents loaded\n",
                        "math: 126,192 documents loaded\n",
                        "physics: 146,311 documents loaded\n",
                        "\n",
                        "Total subjects loaded: 3\n"
                    ]
                }
            ],
            "source": [
                "all_data = {}\n",
                "\n",
                "for subject in LIST_SUBJECT:\n",
                "    df = load_dataset(subject)\n",
                "    if df is not None:\n",
                "        all_data[subject] = df\n",
                "        print(f\"{subject}: {len(df):,} documents loaded\")\n",
                "\n",
                "print(f\"\\nTotal subjects loaded: {len(all_data)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "generate-embeddings",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "============================================================\n",
                        "Processing: CS\n",
                        "============================================================\n",
                        "\n",
                        "[Model: sentence-transformers/all-MiniLM-L6-v2]\n",
                        "  Skipping (already exists): embedding/cs/sentence_transformers_all_MiniLM_L6_v2_v1.mmap\n",
                        "\n",
                        "[Model: all-distilroberta-v1]\n",
                        "  Skipping (already exists): embedding/cs/all_distilroberta_v1_v1.mmap\n",
                        "\n",
                        "[Model: intfloat/e5-base-v2]\n",
                        "  Skipping (already exists): embedding/cs/intfloat_e5_base_v2_v1.mmap\n",
                        "\n",
                        "[Model: all-mpnet-base-v2]\n",
                        "  Skipping (already exists): embedding/cs/all_mpnet_base_v2_v1.mmap\n",
                        "\n",
                        "[Model: BAAI/bge-base-en-v1.5]\n",
                        "  Skipping (already exists): embedding/cs/BAAI_bge_base_en_v1.5_v1.mmap\n",
                        "\n",
                        "[Model: allenai/specter2]\n",
                        "  Skipping (already exists): embedding/cs/allenai_specter2_v1.mmap\n",
                        "\n",
                        "============================================================\n",
                        "Processing: MATH\n",
                        "============================================================\n",
                        "\n",
                        "[Model: sentence-transformers/all-MiniLM-L6-v2]\n",
                        "  Skipping (already exists): embedding/math/sentence_transformers_all_MiniLM_L6_v2_v1.mmap\n",
                        "\n",
                        "[Model: all-distilroberta-v1]\n",
                        "  Skipping (already exists): embedding/math/all_distilroberta_v1_v1.mmap\n",
                        "\n",
                        "[Model: intfloat/e5-base-v2]\n",
                        "  Skipping (already exists): embedding/math/intfloat_e5_base_v2_v1.mmap\n",
                        "\n",
                        "[Model: all-mpnet-base-v2]\n",
                        "  Skipping (already exists): embedding/math/all_mpnet_base_v2_v1.mmap\n",
                        "\n",
                        "[Model: BAAI/bge-base-en-v1.5]\n",
                        "  Skipping (already exists): embedding/math/BAAI_bge_base_en_v1.5_v1.mmap\n",
                        "\n",
                        "[Model: allenai/specter2]\n",
                        "  Skipping (already exists): embedding/math/allenai_specter2_v1.mmap\n",
                        "\n",
                        "============================================================\n",
                        "Processing: PHYSICS\n",
                        "============================================================\n",
                        "\n",
                        "[Model: sentence-transformers/all-MiniLM-L6-v2]\n",
                        "  Total documents: 146,311\n",
                        "  Embedding dimension: 384\n",
                        "  Batch size: 128\n",
                        "  Output path: embedding/physics/sentence_transformers_all_MiniLM_L6_v2_v1.mmap\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  Generating: 100%|██████████| 1144/1144 [03:20<00:00,  5.71it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ✓ Done\n",
                        "\n",
                        "[Model: all-distilroberta-v1]\n",
                        "  Total documents: 146,311\n",
                        "  Embedding dimension: 768\n",
                        "  Batch size: 128\n",
                        "  Output path: embedding/physics/all_distilroberta_v1_v1.mmap\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  Generating: 100%|██████████| 1144/1144 [11:25<00:00,  1.67it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ✓ Done\n",
                        "\n",
                        "[Model: intfloat/e5-base-v2]\n",
                        "  Total documents: 146,311\n",
                        "  Embedding dimension: 768\n",
                        "  Batch size: 128\n",
                        "  Output path: embedding/physics/intfloat_e5_base_v2_v1.mmap\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  Generating: 100%|██████████| 1144/1144 [22:40<00:00,  1.19s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ✓ Done\n",
                        "\n",
                        "[Model: all-mpnet-base-v2]\n",
                        "  Total documents: 146,311\n",
                        "  Embedding dimension: 768\n",
                        "  Batch size: 128\n",
                        "  Output path: embedding/physics/all_mpnet_base_v2_v1.mmap\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  Generating: 100%|██████████| 1144/1144 [26:51<00:00,  1.41s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ✓ Done\n",
                        "\n",
                        "[Model: BAAI/bge-base-en-v1.5]\n",
                        "  Total documents: 146,311\n",
                        "  Embedding dimension: 768\n",
                        "  Batch size: 128\n",
                        "  Output path: embedding/physics/BAAI_bge_base_en_v1.5_v1.mmap\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  Generating: 100%|██████████| 1144/1144 [22:43<00:00,  1.19s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ✓ Done\n",
                        "\n",
                        "[Model: allenai/specter2]\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "128d853a29a74ef2be3dd770598ce1b7",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "There are adapters available but none are activated for the forward pass.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  Total documents: 146,311\n",
                        "  Embedding dimension: 768\n",
                        "  Batch size: 128\n",
                        "  Output path: embedding/physics/allenai_specter2_v1.mmap\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  Generating: 100%|██████████| 1144/1144 [34:09<00:00,  1.79s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  ✓ Done\n",
                        "\n",
                        "============================================================\n",
                        "All embeddings generated!\n",
                        "============================================================\n"
                    ]
                }
            ],
            "source": [
                "for subject, df in all_data.items():\n",
                "    print(f\"\\n{'='*60}\")\n",
                "    print(f\"Processing: {subject.upper()}\")\n",
                "    print(f\"{'='*60}\")\n",
                "    \n",
                "    subject_output_dir = OUTPUT_DIR / subject\n",
                "    subject_output_dir.mkdir(parents=True, exist_ok=True)\n",
                "    \n",
                "    texts = df[\"text\"].fillna(\"\").tolist()\n",
                "    \n",
                "    for model_name in TRANSFORMERS:\n",
                "        print(f\"\\n[Model: {model_name}]\")\n",
                "        \n",
                "        model_safe_name = get_model_safe_name(model_name)\n",
                "        output_path = str(subject_output_dir / f\"{model_safe_name}_{VERSION}.mmap\")\n",
                "        \n",
                "        if os.path.exists(output_path):\n",
                "            print(f\"  Skipping (already exists): {output_path}\")\n",
                "            continue\n",
                "        \n",
                "        try:\n",
                "            if model_name == \"allenai/specter2\":\n",
                "                model, tokenizer = load_specter2()\n",
                "                N = len(texts)\n",
                "                emb_dim = 768\n",
                "                \n",
                "                print(f\"  Total documents: {N:,}\")\n",
                "                print(f\"  Embedding dimension: {emb_dim}\")\n",
                "                print(f\"  Batch size: {BATCH_SIZE}\")\n",
                "                print(f\"  Output path: {output_path}\")\n",
                "                \n",
                "                embeddings = encode_specter2(texts, model, tokenizer, batch_size=BATCH_SIZE)\n",
                "                \n",
                "                embs = np.memmap(output_path, dtype=\"float32\", mode=\"w+\", shape=(N, emb_dim))\n",
                "                embs[:] = embeddings\n",
                "                embs.flush()\n",
                "                \n",
                "                np.save(output_path.replace(\".mmap\", \"_meta.npy\"), {\"n_samples\": N, \"emb_dim\": emb_dim, \"dtype\": \"float32\"})\n",
                "                \n",
                "                del model, tokenizer\n",
                "            elif model_name == \"nomic-ai/nomic-embed-text-v1.5\":\n",
                "                model = SentenceTransformer(model_name, trust_remote_code=True)\n",
                "                generate_mmap_embeddings(texts=texts, embedding_model=model, mmap_path=output_path, batch_size=BATCH_SIZE)\n",
                "                del model\n",
                "            else:\n",
                "                model = SentenceTransformer(model_name)\n",
                "                generate_mmap_embeddings(texts=texts, embedding_model=model, mmap_path=output_path, batch_size=BATCH_SIZE)\n",
                "                del model\n",
                "            \n",
                "            print(f\"  ✓ Done\")\n",
                "            \n",
                "            import gc\n",
                "            gc.collect()\n",
                "            if torch.cuda.is_available():\n",
                "                torch.cuda.empty_cache()\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"  ✗ Error: {e}\")\n",
                "\n",
                "\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"All embeddings generated!\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "verify-output",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generated embedding files:\n",
                        "\n",
                        "cs/\n",
                        "  └── BAAI_bge_base_en_v1.5_meta_v1.npy (0.0 MB)\n",
                        "  └── BAAI_bge_base_en_v1.5_v1.mmap (485.6 MB)\n",
                        "  └── BAAI_bge_base_en_v1.5_v2.mmap (484.9 MB)\n",
                        "  └── BAAI_bge_base_en_v1.5_v2_meta.npy (0.0 MB)\n",
                        "  └── all_distilroberta_v1_meta_v1.npy (0.0 MB)\n",
                        "  └── all_distilroberta_v1_v1.mmap (485.6 MB)\n",
                        "  └── all_distilroberta_v1_v2.mmap (484.9 MB)\n",
                        "  └── all_distilroberta_v1_v2_meta.npy (0.0 MB)\n",
                        "  └── all_mpnet_base_v2_meta_v1.npy (0.0 MB)\n",
                        "  └── all_mpnet_base_v2_v1.mmap (485.6 MB)\n",
                        "  └── all_mpnet_base_v2_v2.mmap (484.9 MB)\n",
                        "  └── all_mpnet_base_v2_v2_meta.npy (0.0 MB)\n",
                        "  └── allenai_specter2_v1.mmap (485.6 MB)\n",
                        "  └── allenai_specter2_v1_meta.npy (0.0 MB)\n",
                        "  └── allenai_specter2_v2.mmap (484.9 MB)\n",
                        "  └── allenai_specter2_v2_meta.npy (0.0 MB)\n",
                        "  └── intfloat_e5_base_v2_meta_v1.npy (0.0 MB)\n",
                        "  └── intfloat_e5_base_v2_v1.mmap (485.6 MB)\n",
                        "  └── intfloat_e5_base_v2_v2.mmap (484.9 MB)\n",
                        "  └── intfloat_e5_base_v2_v2_meta.npy (0.0 MB)\n",
                        "  └── sentence_transformers_all_MiniLM_L6_v2_meta_v1.npy (0.0 MB)\n",
                        "  └── sentence_transformers_all_MiniLM_L6_v2_v1.mmap (242.8 MB)\n",
                        "  └── sentence_transformers_all_MiniLM_L6_v2_v2.mmap (242.4 MB)\n",
                        "  └── sentence_transformers_all_MiniLM_L6_v2_v2_meta.npy (0.0 MB)\n",
                        "math/\n",
                        "  └── BAAI_bge_base_en_v1.5_meta_v1.npy (0.0 MB)\n",
                        "  └── BAAI_bge_base_en_v1.5_v1.mmap (369.7 MB)\n",
                        "  └── BAAI_bge_base_en_v1.5_v2.mmap (363.0 MB)\n",
                        "  └── BAAI_bge_base_en_v1.5_v2_meta.npy (0.0 MB)\n",
                        "  └── all_distilroberta_v1_meta_v1.npy (0.0 MB)\n",
                        "  └── all_distilroberta_v1_v1.mmap (369.7 MB)\n",
                        "  └── all_distilroberta_v1_v2.mmap (363.0 MB)\n",
                        "  └── all_distilroberta_v1_v2_meta.npy (0.0 MB)\n",
                        "  └── all_mpnet_base_v2_meta_v1.npy (0.0 MB)\n",
                        "  └── all_mpnet_base_v2_v1.mmap (369.7 MB)\n",
                        "  └── all_mpnet_base_v2_v2.mmap (363.0 MB)\n",
                        "  └── all_mpnet_base_v2_v2_meta.npy (0.0 MB)\n",
                        "  └── allenai_specter2_v1.mmap (369.7 MB)\n",
                        "  └── allenai_specter2_v1_meta.npy (0.0 MB)\n",
                        "  └── allenai_specter2_v2.mmap (363.0 MB)\n",
                        "  └── allenai_specter2_v2_meta.npy (0.0 MB)\n",
                        "  └── intfloat_e5_base_v2_meta_v1.npy (0.0 MB)\n",
                        "  └── intfloat_e5_base_v2_v1.mmap (369.7 MB)\n",
                        "  └── intfloat_e5_base_v2_v2.mmap (363.0 MB)\n",
                        "  └── intfloat_e5_base_v2_v2_meta.npy (0.0 MB)\n",
                        "  └── sentence_transformers_all_MiniLM_L6_v2_meta_v1.npy (0.0 MB)\n",
                        "  └── sentence_transformers_all_MiniLM_L6_v2_v1.mmap (184.9 MB)\n",
                        "  └── sentence_transformers_all_MiniLM_L6_v2_v2.mmap (181.5 MB)\n",
                        "  └── sentence_transformers_all_MiniLM_L6_v2_v2_meta.npy (0.0 MB)\n",
                        "physics/\n",
                        "  └── BAAI_bge_base_en_v1.5_v1.mmap (428.6 MB)\n",
                        "  └── BAAI_bge_base_en_v1.5_v1_meta.npy (0.0 MB)\n",
                        "  └── all_distilroberta_v1_v1.mmap (428.6 MB)\n",
                        "  └── all_distilroberta_v1_v1_meta.npy (0.0 MB)\n",
                        "  └── all_mpnet_base_v2_v1.mmap (428.6 MB)\n",
                        "  └── all_mpnet_base_v2_v1_meta.npy (0.0 MB)\n",
                        "  └── all_mpnet_base_v2_v2.mmap (427.4 MB)\n",
                        "  └── all_mpnet_base_v2_v2_meta.npy (0.0 MB)\n",
                        "  └── allenai_specter2_v1.mmap (428.6 MB)\n",
                        "  └── allenai_specter2_v1_meta.npy (0.0 MB)\n",
                        "  └── allenai_specter2_v2.mmap (427.4 MB)\n",
                        "  └── allenai_specter2_v2_meta.npy (0.0 MB)\n",
                        "  └── intfloat_e5_base_v2_v1.mmap (428.6 MB)\n",
                        "  └── intfloat_e5_base_v2_v1_meta.npy (0.0 MB)\n",
                        "  └── intfloat_e5_base_v2_v2.mmap (427.4 MB)\n",
                        "  └── intfloat_e5_base_v2_v2_meta.npy (0.0 MB)\n",
                        "  └── sentence_transformers_all_MiniLM_L6_v2_v1.mmap (214.3 MB)\n",
                        "  └── sentence_transformers_all_MiniLM_L6_v2_v1_meta.npy (0.0 MB)\n",
                        "  └── sentence_transformers_all_MiniLM_L6_v2_v2.mmap (213.7 MB)\n",
                        "  └── sentence_transformers_all_MiniLM_L6_v2_v2_meta.npy (0.0 MB)\n"
                    ]
                }
            ],
            "source": [
                "print(\"Generated embedding files:\\n\")\n",
                "\n",
                "for subject in LIST_SUBJECT:\n",
                "    subject_dir = OUTPUT_DIR / subject\n",
                "    if subject_dir.exists():\n",
                "        print(f\"{subject}/\")\n",
                "        for f in sorted(subject_dir.iterdir()):\n",
                "            size_mb = f.stat().st_size / (1024 * 1024)\n",
                "            print(f\"  └── {f.name} ({size_mb:.1f} MB)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
